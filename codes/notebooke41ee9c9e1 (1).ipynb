{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11748123,"sourceType":"datasetVersion","datasetId":7375130},{"sourceId":11835167,"sourceType":"datasetVersion","datasetId":7435483},{"sourceId":11837927,"sourceType":"datasetVersion","datasetId":7437465}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\nimport json\nfrom glob import glob\n\n# Load listing metadata\njson_dir = '/kaggle/input/vrmini2/abo-listings/listings/metadata/'\njson_files = glob(os.path.join(json_dir, 'listings_*.json'))\n\ndataframes = []\nfor file in json_files:\n    with open(file, 'r') as f:\n        lines = f.readlines()\n        records = [json.loads(line.strip()) for line in lines if line.strip()]\n        df = pd.json_normalize(records)\n        dataframes.append(df)\n\nlisting_df = pd.concat(dataframes, ignore_index=True)\n\n# Load image metadata\nimage_df = pd.read_csv('/kaggle/input/vrmini2/abo-images-small/images/metadata/images.csv')\n\n# Rename 'main_image_id' to 'image_id' in listing_df for matching\nlisting_df_renamed = listing_df.rename(columns={'main_image_id': 'image_id'})\n\n# Merge listing and image metadata on image_id\ncombined_metadata2 = pd.merge(listing_df_renamed, image_df, on='image_id', how='inner')\n\n# Export the combined DataFrame to a CSV file\ncombined_metadata2.to_csv('combined_metadata2.csv', index=False)\n\nprint(\"Combined metadata saved to 'combined_metadata2.csv'.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your file\ndf = pd.read_csv('combined_metadata2.csv')\n\n# Add full_path by prepending the image base directory\nbase_path = '/kaggle/input/vrmini2/abo-images-small/images/small/'\ndf['full_path'] = base_path + df['path'].astype(str)\n\n# Save the updated DataFrame\ndf.to_csv('/kaggle/working/combined_metadata2_with_full_path.csv', index=False)\n\nprint(\"✅ full_path added and saved as combined_metadata2_with_full_path.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nimport base64\nimport pandas as pd\nfrom tqdm import tqdm\nimport time\nfrom google.generativeai import configure, GenerativeModel\nfrom google.api_core.exceptions import ResourceExhausted, ServiceUnavailable\n\n# Configuration variables - modify these as needed\nINPUT_FILE = 'combined_metadata2_with_full_path.csv'\nOUTPUT_FILE = 'vqa_training_data_9.csv'  # Changed to CSV output\nAPI_KEY = 'AIzaSyDEgRHY6uWUC76xr99V2pzHbwDexyoEbKM'  # Replace with your actual Gemini API key\nIMAGE_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"\nRETRY_ATTEMPTS = 3\nDELAY = 4.5  # Warning: This risks hitting the 15 RPM and 1,500 RPD limits quickly; consider 60.0 for safety\n\n# Range variables\nSTART_INDEX = 53078\nEND_INDEX = 53578\nAPPEND_RESULTS = True\n\ndef encode_image(image_path):\n    \"\"\"Encode an image file to base64 string.\"\"\"\n    try:\n        with open(image_path, \"rb\") as image_file:\n            return base64.b64encode(image_file.read()).decode('utf-8')\n    except Exception as e:\n        print(f\"Error encoding image {image_path}: {e}\")\n        return None\n\ndef generate_vqa_data(model, metadata, image_path, retry_attempts=RETRY_ATTEMPTS, delay=DELAY):\n    \"\"\"Generate VQA data for a single product using Gemini API.\"\"\"\n    prompt = \"\"\"You are an AI assistant helping to generate training data for a Visual Question Answering (VQA) model.\nYou are provided with:\n- A product image\n- Detailed product metadata (brand, style, color, features, description, etc.)\n\nYour task is to generate diverse and meaningful questions that require both visual understanding and contextual reasoning from the metadata. The goal is to help train a robust VQA model that generalizes well to unseen product types and questions.\n\nUse both the image and the metadata together to craft the questions. Make sure each question is visually answerable using the image while being enhanced by the metadata. Do not copy metadata text directly into answers — paraphrase or infer instead. Encourage variety in question types and phrasing. Avoid overfitting by ensuring questions are not repeated across images or overly templated.\n\nGuidelines:\n- Generate exactly 3 diverse questions per image.\n- Questions must be answerable based on the image, optionally supported by metadata.\n- Keep answers short and specific (1 word max).\n- Use a mix of question types as appropriate for the image:\n  - Descriptive\n  - Counting\n  - Comparative\n  - Color recognition\n  - Function-based\n  - Reasoning-based\n\nOutput Format (strict JSON format):\n{\n  \"image_id\": \"IMAGE_ID_HERE\",\n  \"questions\": [\n    {\n      \"question\": \"QUESTION TEXT HERE\",\n      \"answer\": \"ANSWER HERE\"\n    },\n    {\n      \"question\": \"QUESTION TEXT HERE\",\n      \"answer\": \"ANSWER HERE\"\n    },\n    {\n      \"question\": \"QUESTION TEXT HERE\",\n      \"answer\": \"ANSWER HERE\"\n    }\n  ]\n}\n\nProduct Metadata:\n\"\"\"\n    prompt += json.dumps(metadata, indent=2)\n    \n    # Encode image to base64\n    image_data = encode_image(image_path)\n    if not image_data:\n        return None\n    \n    # Prepare the image for the API\n    image_parts = [\n        {\n            \"mime_type\": \"image/jpeg\",\n            \"data\": image_data\n        }\n    ]\n    \n    for attempt in range(retry_attempts):\n        try:\n            response = model.generate_content(\n                contents=[\n                    {\"role\": \"user\", \"parts\": [{\"text\": prompt}, {\"inline_data\": image_parts[0]}]}\n                ],\n                generation_config={\n                    \"temperature\": 0.4,\n                    \"max_output_tokens\": 1024,\n                }\n            )\n            \n            # Extract JSON from the response\n            response_text = response.text\n            json_start = response_text.find('{')\n            json_end = response_text.rfind('}') + 1\n            \n            if json_start >= 0 and json_end > json_start:\n                json_str = response_text[json_start:json_end]\n                try:\n                    return json.loads(json_str)\n                except json.JSONDecodeError:\n                    print(f\"Failed to parse JSON response: {json_str}\")\n            else:\n                print(f\"No valid JSON found in response: {response_text}\")\n            \n            if attempt < retry_attempts - 1:\n                print(f\"Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                \n        except (ResourceExhausted, ServiceUnavailable) as e:\n            print(f\"API limit exceeded or service unavailable: {e}\")\n            if attempt < retry_attempts - 1:\n                sleep_time = delay * (2 ** attempt)\n                print(f\"Retrying in {sleep_time} seconds...\")\n                time.sleep(sleep_time)\n            else:\n                print(\"Maximum retry attempts reached.\")\n                return None\n        except Exception as e:\n            print(f\"Error calling Gemini API: {e}\")\n            if attempt < retry_attempts - 1:\n                print(f\"Retrying in {delay} seconds...\")\n                time.sleep(delay)\n            else:\n                return None\n    \n    return None\n\ndef append_to_csv_file(data, filename):\n    \"\"\"Append VQA results to an existing CSV file or create a new one, using full_path instead of image_id.\"\"\"\n    try:\n        # Prepare data as a list of rows for the CSV\n        csv_rows = []\n        for item in data:\n            full_path = item['full_path']  # Use full_path instead of image_id\n            for q in item['questions']:\n                csv_rows.append({\n                    'full_path': full_path,  # Changed column name to full_path\n                    'question': q['question'],\n                    'answer': q['answer']\n                })\n        \n        # Convert to DataFrame\n        df_new = pd.DataFrame(csv_rows)\n        \n        # If file exists and APPEND_RESULTS is True, append to it\n        if APPEND_RESULTS and os.path.exists(filename):\n            df_existing = pd.read_csv(filename)\n            # If the existing file has an 'image_id' column, rename it to 'full_path' for consistency\n            if 'image_id' in df_existing.columns:\n                df_existing = df_existing.rename(columns={'image_id': 'full_path'})\n            df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n            df_combined.to_csv(filename, index=False)\n        else:\n            df_new.to_csv(filename, index=False)\n        \n        return True\n    except Exception as e:\n        print(f\"Error writing to CSV file: {e}\")\n        return False\n\ndef main():\n    # Configure Gemini API\n    configure(api_key=API_KEY)\n    model = GenerativeModel('gemini-1.5-flash')\n    \n    # Read the input CSV file\n    try:\n        df = pd.read_csv(INPUT_FILE)\n        \n        # Handle the range of items to process\n        start_idx = START_INDEX\n        end_idx = END_INDEX if END_INDEX is not None else len(df)\n        \n        # Validate range\n        if start_idx < 0:\n            start_idx = 0\n        if end_idx > len(df):\n            end_idx = len(df)\n        if start_idx >= end_idx:\n            print(f\"Invalid range: start ({start_idx}) must be less than end ({end_idx})\")\n            return\n        \n        # Select the range of items to process\n        data_to_process = df.iloc[start_idx:end_idx]\n        \n        if data_to_process.empty:\n            print(\"No valid data found in the specified range.\")\n            return\n        \n        print(f\"Processing items from index {start_idx} to {end_idx-1} ({len(data_to_process)} items)\")\n    except Exception as e:\n        print(f\"Error reading input file: {e}\")\n        return\n    \n    results = []\n    \n    # Process each row in the DataFrame\n    for idx, row in enumerate(tqdm(data_to_process.iterrows(), total=len(data_to_process), desc=\"Processing items\")):\n        try:\n            row_data = row[1]\n            item_id = row_data['image_id']\n            metadata = row_data.drop(['full_path', 'path']).to_dict()\n            image_path = row_data['full_path']\n            \n            print(f\"Processing item {start_idx + idx} (ID: {item_id})\")\n            \n            # Generate VQA data\n            vqa_data = generate_vqa_data(model, metadata, image_path, delay=DELAY)\n            \n            if vqa_data:\n                vqa_data['image_id'] = item_id\n                vqa_data['full_path'] = image_path  # Add full_path to vqa_data for CSV output\n                results.append(vqa_data)\n                \n                time.sleep(DELAY)\n        except Exception as e:\n            print(f\"Error processing item {item_id}: {e}\")\n    \n    # Write results to output file\n    try:\n        if results:\n            success = append_to_csv_file(results, OUTPUT_FILE)\n            if success:\n                print(f\"Saved VQA data for {len(results)} items to {OUTPUT_FILE}\")\n        else:\n            print(\"No VQA data generated to save.\")\n    except Exception as e:\n        print(f\"Error writing output file: {e}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# blip2_base_raw.py\n\n# Import required libraries\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions_blip2.csv\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (replace with your actual token)\nhf_token = \"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\"  # Replace with your Hugging Face API token\n\n# Load BLIP-2\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", token=hf_token)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, token=hf_token\n).to(device)\n\n# Load dataset\ntrain_df = pd.read_csv(\"/kaggle/input/vr1234/train_split.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vr1234/test_split.csv\")\n\n# Evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\ntest_df['correct'] = test_df['predicted_answer'].str.strip().str.lower() == test_df['answer'].str.strip().str.lower()\n\n# Save predictions to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# Evaluation Metrics\ntqdm.pandas()\n\n# Load predictions\ndf = pd.read_csv(\"vqa_test_predictions_blip2.csv\")\n\n# Normalize text\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Token-level macro F1\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\n# Print metrics\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install transformers torch pandas tqdm Pillow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# blip2_base_raw.py\n\n# Install evaluation dependencies\nimport subprocess\nimport sys\n\ndef install_packages():\n    packages = ['bert-score', 'rouge-score', 'nltk']\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            sys.exit(1)\n\ninstall_packages()\n\n# Import required libraries\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions_blip2.csv\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (replace with your actual token)\nhf_token = \"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\"  # Replace this with your Hugging Face API token\n\n# Load BLIP-2 with the token\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", token=hf_token)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16, token=hf_token\n).to(device)\n\n# Load dataset\ntrain_df = pd.read_csv(\"/kaggle/input/vr1234/train_split.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vr1234/test_split.csv\")\n\n# Evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\ntest_df['correct'] = test_df['predicted_answer'].str.strip().str.lower() == test_df['answer'].str.strip().str.lower()\n\n# Save predictions to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# Evaluation Metrics\ntqdm.pandas()\n\n# Load predictions\ndf = pd.read_csv(\"vqa_test_predictions_blip2.csv\")\n\n# Normalize text\ndef normalize_text(s):\n    return str(s).strip().lower()\n\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Token-level macro F1\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\n# Print metrics\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# blip2_base_raw.py\n\n# Set environment variable to reduce memory fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Install evaluation dependencies\nimport subprocess\nimport sys\nimport re\nimport torch\n\ndef install_packages():\n    packages = ['bert-score', 'rouge-score', 'nltk']\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            sys.exit(1)\n\ninstall_packages()\n\n# Import required libraries\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions_blip2.csv\")\n\n# Device setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (replace with your actual token)\nhf_token = \"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\"  # Replace this with your Hugging Face API token\n\n# Load BLIP-2 with the correct model ID\nprocessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", token=hf_token)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n).to(device)\n\n# Load dataset\ntrain_df = pd.read_csv(\"/kaggle/input/vr1234/train_split.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vr1234/test_split.csv\")\n\n# Post-process model output\ndef post_process_answer(answer, question):\n    answer = answer.strip().lower()\n    # For yes/no questions, extract key words\n    if \"yes or no\" in question.lower() or \"is \" in question.lower():\n        if \"yes\" in answer:\n            return \"yes\"\n        if \"no\" in answer:\n            return \"no\"\n    return answer\n\n# Evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        answer = post_process_answer(answer, question)\n        torch.cuda.empty_cache()  # Free memory after each prediction\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\n\n# Improved normalization\ndef normalize_text(s):\n    s = str(s).strip().lower()\n    s = re.sub(r'[^\\w\\s]', '', s)  # Remove punctuation\n    s = re.sub(r'\\s+', ' ', s)     # Normalize internal whitespace\n    return s\n\n# Normalize answers\ntest_df['answer'] = test_df['answer'].astype(str).apply(normalize_text)\ntest_df['predicted_answer'] = test_df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Compute accuracy\ntest_df['correct'] = test_df['answer'] == test_df['predicted_answer']\n\n# Debug mismatches\nmismatches = test_df[~test_df['correct']].head(5)  # First 5 mismatches\nfor idx, row in mismatches.iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"Ground Truth: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(\"---\")\n\n# Save predictions to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# Evaluation Metrics\ntqdm.pandas()\n\n# Load predictions\ndf = pd.read_csv(\"vqa_test_predictions_blip2.csv\")\n\n# Normalize again for evaluation\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Token-level macro F1\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# Accuracy\naccuracy = (df['answer'] == df['predicted_answer']).mean()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\n# Print metrics\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# blip2_base_raw.py\n\n# Set environment variable to reduce memory fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Install evaluation dependencies\nimport subprocess\nimport sys\nimport re\nimport torch\n\ndef install_packages():\n    packages = ['bert-score', 'rouge-score', 'nltk']\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            sys.exit(1)\n\ninstall_packages()\n\n# Import required libraries\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions_blip2.csv\")\n\n# Device setup with fallback to CPU\ntry:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\nexcept Exception as e:\n    print(f\"Error setting device: {e}, defaulting to CPU\")\n    device = torch.device(\"cpu\")\n\n# Hugging Face token\nhf_token = \"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\"\n\n# Load BLIP-2 with the correct model ID\ntry:\n    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", token=hf_token)\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\nexcept torch.cuda.OutOfMemoryError:\n    print(\"CUDA out of memory, switching to CPU...\")\n    device = torch.device(\"cpu\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\n\n# Load dataset with error handling\ntry:\n    train_df = pd.read_csv(\"/kaggle/input/vr1234/train_split.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/vr1234/test_split.csv\")\nexcept FileNotFoundError as e:\n    print(f\"Dataset not found: {e}\")\n    sys.exit(1)\n\n# Post-process model output (only for yes/no questions, since prompt should ensure 1-word answers)\ndef post_process_answer(answer, question):\n    answer = answer.strip().lower()\n    # For yes/no questions, ensure correct extraction\n    if \"yes or no\" in question.lower() or \"is \" in question.lower():\n        if \"yes\" in answer:\n            return \"yes\"\n        if \"no\" in answer:\n            return \"no\"\n    return answer  # Return the answer as-is, expecting it to be 1 word due to prompt\n\n# Evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        # Prepend prompt to encourage 1-word answer\n        question = \"Answer in one word: \" + row['question']\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        # Ensure both image and question are processed\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs, max_new_tokens=5)  # Limit output length to reduce verbosity\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        answer = post_process_answer(answer, question)\n        torch.cuda.empty_cache()  # Free memory after each prediction\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\n\n# Improved normalization\ndef normalize_text(s):\n    s = str(s).strip().lower()\n    s = re.sub(r'[^\\w\\s]', '', s)  # Remove punctuation\n    s = re.sub(r'\\s+', ' ', s)     # Normalize internal whitespace\n    return s\n\n# Normalize answers\ntest_df['answer'] = test_df['answer'].astype(str).apply(normalize_text)\ntest_df['predicted_answer'] = test_df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Compute accuracy with relaxed matching\ntest_df['correct'] = test_df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n)\n\n# Debug mismatches\nmismatches = test_df[~test_df['correct']].head(5)  # First 5 mismatches\nfor idx, row in mismatches.iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"Ground Truth: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(\"---\")\n\n# Save predictions to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# Evaluation Metrics\ntqdm.pandas()\n\n# Load predictions\ndf = pd.read_csv(\"vqa_test_predictions_blip2.csv\")\n\n# Normalize again for evaluation\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Token-level macro F1\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# Accuracy with relaxed matching\naccuracy = df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n).mean()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\n# Print metrics\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set environment variable to reduce memory fragmentation\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Install evaluation dependencies\nimport subprocess\nimport sys\nimport re\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration, Trainer, TrainingArguments\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\ndef install_packages():\n    packages = ['bert-score', 'rouge-score', 'nltk']\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            sys.exit(1)\n\ninstall_packages()\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions_blip3_compatible.csv\")\n\n# Device setup with fallback to CPU\ntry:\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\nexcept Exception as e:\n    print(f\"Error setting device: {e}, defaulting to CPU\")\n    device = torch.device(\"cpu\")\n\n# Hugging Face token (replace with your valid token if needed)\nhf_token = \"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\"\n\n# Load BLIP-2 with the correct model ID\ntry:\n    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", token=hf_token)\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\nexcept torch.cuda.OutOfMemoryError:\n    print(\"CUDA out of memory, switching to CPU...\")\n    device = torch.device(\"cpu\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\n\n# Load the train and test datasets from vr1234\ntry:\n    train_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\nexcept FileNotFoundError as e:\n    print(f\"Dataset not found: {e}\")\n    print(\"Please ensure the CSV files are at /kaggle/input/vr4567/more_and_12049_train.csv and /kaggle/input/vr4567/more_and_12049_test.csv\")\n    sys.exit(1)\n\n# Verify that images are accessible from vrmini2\nprint(\"Sample image paths from training dataset:\")\nprint(train_df['full_path'].head(5))\nprint(\"Ensure these paths exist in /kaggle/input/vrmini2/abo-images-small/images/small/\")\n\n# Analyze dataset distribution for better defaults\nprint(\"Most common answers in training set:\")\nprint(train_df['answer'].value_counts().head(10))\n\n# Expanded vocabulary of common answers\ncommon_answers = {\n    \"colors\": [\"white\", \"black\", \"blue\", \"red\", \"green\", \"yellow\", \"pink\", \"purple\", \"orange\", \"grey\", \"gray\", \"brown\", \"beige\", \"tan\", \"silver\", \"gold\", \"crimson\", \"navy\", \"violet\"],\n    \"materials\": [\"plastic\", \"silicone\", \"metal\", \"wood\", \"leather\", \"suede\", \"cotton\", \"polyester\", \"fabric\", \"synthetic\", \"brass\", \"steel\", \"canvas\", \"cloth\", \"rubber\"],\n    \"yes_no\": [\"yes\", \"no\"],\n    \"numbers\": [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"],\n    \"phone_models\": [\"iphone\", \"samsung\", \"vivo\", \"oppo\", \"motorola\", \"lenovo\", \"redmi\", \"note\", \"moto\", \"lg\", \"nokia\"],\n    \"styles\": [\"modern\", \"floral\", \"geometric\", \"abstract\", \"minimalist\", \"rustic\", \"classic\", \"ankle\", \"block\", \"sling\", \"vintage\"],\n    \"objects\": [\"heart\", \"skull\", \"flowers\", \"moon\", \"stars\", \"cat\", \"dog\", \"lion\", \"butterflies\", \"teddy\", \"tree\"]\n}\n\n# Flatten the vocabulary for post-processing\nvalid_answers = set()\nfor category in common_answers.values():\n    valid_answers.update(category)\n\n# Synonym mapping\nsynonym_map = {\n    \"grey\": \"gray\",\n    \"tan\": \"beige\",\n    \"cloth\": \"fabric\",\n    \"crimson\": \"red\",\n    \"navy\": \"blue\",\n    \"violet\": \"purple\"\n}\n\n# Custom Dataset for fine-tuning\nclass VQADataset(torch.utils.data.Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row['full_path']\n        question = row['question']\n        # Adjust prompt based on question type during training\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        answer = row['answer']\n\n        # Load and process image from vrmini2\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Failed to load image {image_path}: {e}\")\n            # Create a blank image as a fallback\n            image = Image.new('RGB', (224, 224), color='gray')\n        \n        inputs = self.processor(images=image, text=full_prompt, return_tensors=\"pt\", padding=True)\n\n        # Process the answer (target)\n        labels = self.processor.tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n\n        # Remove batch dimension\n        for key in inputs:\n            inputs[key] = inputs[key].squeeze(0)\n        labels = labels.squeeze(0)\n\n        return {\"pixel_values\": inputs[\"pixel_values\"], \"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": labels}\n\n# Function to classify question type and adjust prompt\ndef get_question_type(question):\n    question = question.lower()\n    if \"color\" in question or \"colour\" in question:\n        return \"color\", \"Answer with a color (e.g., red, blue, green): \"\n    elif \"material\" in question:\n        return \"material\", \"Answer with a material (e.g., plastic, leather, metal): \"\n    elif \"yes or no\" in question or \"is \" in question or \"are \" in question:\n        return \"yes_no\", \"Answer with yes or no: \"\n    elif \"how many\" in question:\n        return \"number\", \"Answer with a number (e.g., one, two, three): \"\n    elif \"phone\" in question or \"model\" in question:\n        return \"phone_model\", \"Answer with a phone model (e.g., iPhone, Samsung): \"\n    elif \"style\" in question or \"pattern\" in question:\n        return \"style\", \"Answer with a style or pattern (e.g., floral, modern): \"\n    else:\n        return \"object\", \"Answer with an object (e.g., heart, skull): \"\n\n# Improved post-processing\ndef post_process_answer(answer, question, q_type):\n    answer = answer.strip().lower()\n    # If multi-word, take the first relevant word\n    answer_words = answer.split()\n    answer = answer_words[0] if answer_words else answer\n    # Apply synonym mapping\n    answer = synonym_map.get(answer, answer)\n    \n    # Handle based on question type\n    if q_type == \"yes_no\":\n        if \"yes\" in answer:\n            return \"yes\"\n        if \"no\" in answer:\n            return \"no\"\n        return \"yes\"  # Default for yes/no questions\n    elif q_type == \"color\":\n        if answer in common_answers[\"colors\"]:\n            return answer\n        return \"black\"  # Default color (adjust based on dataset distribution)\n    elif q_type == \"material\":\n        if answer in common_answers[\"materials\"]:\n            return answer\n        return \"plastic\"  # Default material\n    elif q_type == \"number\":\n        if answer in common_answers[\"numbers\"]:\n            return answer\n        return \"two\"  # Default number\n    elif q_type == \"phone_model\":\n        if answer in common_answers[\"phone_models\"]:\n            return answer\n        return \"iphone\"  # Default phone model\n    elif q_type == \"style\":\n        if answer in common_answers[\"styles\"]:\n            return answer\n        return \"modern\"  # Default style\n    elif q_type == \"object\":\n        if answer in common_answers[\"objects\"]:\n            return answer\n        return \"heart\"  # Default object\n    \n    # Fallback for unknown question types\n    if answer in valid_answers:\n        return answer\n    return \"yes\"  # Final default\n\n# Split train_df into training and validation sets\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\ntrain_dataset = VQADataset(train_df, processor)\nval_dataset = VQADataset(val_df, processor)\n\n# Define training arguments for fine-tuning (compatible with older transformers versions)\n# Calculate steps per epoch to evaluate after each epoch\nsteps_per_epoch = len(train_dataset) // 2  # Batch size is 2\ntraining_args = TrainingArguments(\n    output_dir=\"./blip2_finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=1,\n    fp16=True,\n    eval_strategy=\"epoch\",     # Evaluate after each epoch\n    remove_unused_columns=False\n)\n\n# Initialize the Trainer for fine-tuning\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\n# Fine-tune the model\nprint(\"Starting fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning completed.\")\n\n# Evaluate on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        ground_truth = row['answer']\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs, max_new_tokens=3, num_beams=3)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        answer = post_process_answer(answer, question, q_type)\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"yes\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\n\n# Add question type for analysis\ntest_df['question_type'] = test_df['question'].apply(lambda q: get_question_type(q)[0])\n\n# Improved normalization\ndef normalize_text(s):\n    s = str(s).strip().lower()\n    s = re.sub(r'[^\\w\\s]', '', s)  # Remove punctuation\n    s = re.sub(r'\\s+', ' ', s)     # Normalize internal whitespace\n    return s\n\n# Normalize answers\ntest_df['answer'] = test_df['answer'].astype(str).apply(normalize_text)\ntest_df['predicted_answer'] = test_df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Compute overall accuracy with relaxed matching\ntest_df['correct'] = test_df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n)\n\n# Compute accuracy by question type\naccuracy_by_type = test_df.groupby('question_type').apply(\n    lambda df: df['correct'].mean()\n)\nprint(\"\\nAccuracy by question type:\")\nprint(accuracy_by_type)\n\n# Debug mismatches\nmismatches = test_df[~test_df['correct']].head(5)\nfor idx, row in mismatches.iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"Ground Truth: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(\"---\")\n\n# Save predictions to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# Evaluation Metrics\ntqdm.pandas()\n\n# Load predictions\ndf = pd.read_csv(\"vqa_test_predictions_blip3_compatible.csv\")\n\n# Normalize again for evaluation\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\n# Token-level macro F1\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\n\n# Accuracy with relaxed matching\naccuracy = df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n).mean()\n\n# ROUGE Score\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\n# BERTScore (Precision/Recall/F1)\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\n# BLEU Score\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\n# Print metrics\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip show torch\n!pip show transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y torch torchvision transformers\n!pip install torch==2.0.1 torchvision==0.15.2 transformers==4.31.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nimport torch\nprint(torch.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision\nimport transformers\nprint(torch.__version__, torchvision.__version__, transformers.__version__)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Install and Import Dependencies ---\nimport sys\nimport subprocess\n\ndef run(cmd):\n    print(f\"Running: {cmd}\")\n    subprocess.check_call(cmd, shell=True)\n\n# Uninstall current torch and transformers\nrun(f\"{sys.executable} -m pip uninstall -y torch torchvision torchaudio transformers\")\n\n# Install compatible torch and transformers\nrun(f\"{sys.executable} -m pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\")\nrun(f\"{sys.executable} -m pip install transformers==4.40.2\")\n\n# Install other dependencies\nrun(f\"{sys.executable} -m pip install bert-score rouge-score nltk pillow pandas tqdm scikit-learn\")\n\ndef install_packages():\n    packages = [\n        'torch', 'transformers>=4.51.0', 'bert-score', 'rouge-score', 'nltk', \n        'pillow', 'pandas', 'tqdm', 'scikit-learn'\n    ]\n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n        except subprocess.CalledProcessError as e:\n            print(f\"Failed to install {package}: {e}\")\n            sys.exit(1)\n\ninstall_packages()\n\nimport os\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration, Trainer, TrainingArguments\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\n# --- Device Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Hugging Face Token ---\nhf_token = \"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\"  # Replace with your token\n\n# --- Load Model and Processor ---\ntry:\n    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", token=hf_token)\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\nexcept torch.cuda.OutOfMemoryError:\n    print(\"CUDA OOM, switching to CPU...\")\n    device = torch.device(\"cpu\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\n\n# --- Load Data ---\ntry:\n    train_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\nexcept FileNotFoundError as e:\n    print(f\"Dataset not found: {e}\")\n    sys.exit(1)\n\n# --- Common Answers and Synonyms ---\ncommon_answers = {\n    \"colors\": [\"white\", \"black\", \"blue\", \"red\", \"green\", \"yellow\", \"pink\", \"purple\", \"orange\", \"grey\", \"gray\", \"brown\", \"beige\", \"tan\", \"silver\", \"gold\", \"crimson\", \"navy\", \"violet\"],\n    \"materials\": [\"plastic\", \"silicone\", \"metal\", \"wood\", \"leather\", \"suede\", \"cotton\", \"polyester\", \"fabric\", \"synthetic\", \"brass\", \"steel\", \"canvas\", \"cloth\", \"rubber\"],\n    \"yes_no\": [\"yes\", \"no\"],\n    \"numbers\": [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"],\n    \"phone_models\": [\"iphone\", \"samsung\", \"vivo\", \"oppo\", \"motorola\", \"lenovo\", \"redmi\", \"note\", \"moto\", \"lg\", \"nokia\"],\n    \"styles\": [\"modern\", \"floral\", \"geometric\", \"abstract\", \"minimalist\", \"rustic\", \"classic\", \"ankle\", \"block\", \"sling\", \"vintage\"],\n    \"objects\": [\"heart\", \"skull\", \"flowers\", \"moon\", \"stars\", \"cat\", \"dog\", \"lion\", \"butterflies\", \"teddy\", \"tree\"]\n}\nvalid_answers = set()\nfor category in common_answers.values():\n    valid_answers.update(category)\nsynonym_map = {\n    \"grey\": \"gray\", \"tan\": \"beige\", \"cloth\": \"fabric\", \"crimson\": \"red\", \"navy\": \"blue\", \"violet\": \"purple\"\n}\n\n# --- Helper Functions ---\ndef get_question_type(question):\n    question = question.lower()\n    if \"color\" in question or \"colour\" in question:\n        return \"color\", \"Answer with a color (e.g., red, blue, green): \"\n    elif \"material\" in question:\n        return \"material\", \"Answer with a material (e.g., plastic, leather, metal): \"\n    elif \"yes or no\" in question or \"is \" in question or \"are \" in question:\n        return \"yes_no\", \"Answer with yes or no: \"\n    elif \"how many\" in question:\n        return \"number\", \"Answer with a number (e.g., one, two, three): \"\n    elif \"phone\" in question or \"model\" in question:\n        return \"phone_model\", \"Answer with a phone model (e.g., iPhone, Samsung): \"\n    elif \"style\" in question or \"pattern\" in question:\n        return \"style\", \"Answer with a style or pattern (e.g., floral, modern): \"\n    else:\n        return \"object\", \"Answer with an object (e.g., heart, skull): \"\n\ndef post_process_answer(answer, question, q_type):\n    answer = answer.strip().lower()\n    answer_words = answer.split()\n    answer = answer_words[0] if answer_words else answer\n    answer = synonym_map.get(answer, answer)\n    if q_type == \"yes_no\":\n        if \"yes\" in answer:\n            return \"yes\"\n        if \"no\" in answer:\n            return \"no\"\n        return \"yes\"\n    elif q_type == \"color\":\n        if answer in common_answers[\"colors\"]:\n            return answer\n        return \"black\"\n    elif q_type == \"material\":\n        if answer in common_answers[\"materials\"]:\n            return answer\n        return \"plastic\"\n    elif q_type == \"number\":\n        if answer in common_answers[\"numbers\"]:\n            return answer\n        return \"two\"\n    elif q_type == \"phone_model\":\n        if answer in common_answers[\"phone_models\"]:\n            return answer\n        return \"iphone\"\n    elif q_type == \"style\":\n        if answer in common_answers[\"styles\"]:\n            return answer\n        return \"modern\"\n    elif q_type == \"object\":\n        if answer in common_answers[\"objects\"]:\n            return answer\n        return \"heart\"\n    if answer in valid_answers:\n        return answer\n    return \"yes\"\n\ndef normalize_text(s):\n    s = str(s).strip().lower()\n    s = re.sub(r'[^\\w\\s]', '', s)\n    s = re.sub(r'\\s+', ' ', s)\n    return s\n\n# --- Custom Dataset ---\nclass VQADataset(torch.utils.data.Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row['full_path']\n        question = row['question']\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        answer = row['answer']\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Failed to load image {image_path}: {e}\")\n            image = Image.new('RGB', (224, 224), color='gray')\n        inputs = self.processor(images=image, text=full_prompt, return_tensors=\"pt\", padding=True)\n        labels = self.processor.tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n        for key in inputs:\n            inputs[key] = inputs[key].squeeze(0)\n        labels = labels.squeeze(0)\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"], \n            \"input_ids\": inputs[\"input_ids\"], \n            \"attention_mask\": inputs[\"attention_mask\"], \n            \"labels\": labels\n        }\n\n# --- Data Split ---\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\ntrain_dataset = VQADataset(train_df, processor)\nval_dataset = VQADataset(val_df, processor)\n\n# --- Training Arguments (transformers >= 4.51.0 uses 'eval_strategy') ---\nsteps_per_epoch = len(train_dataset) // 2  # Batch size is 2\ntraining_args = TrainingArguments(\n    output_dir=\"./blip2_finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=1,\n    fp16=True,\n    eval_strategy=\"epoch\",   # <-- correct for transformers >= 4.51.0\n    remove_unused_columns=False\n)\n\n# --- Trainer ---\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\nprint(\"Starting fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning completed.\")\n\n# --- Inference on Test Set ---\npredictions = []\nreferences = test_df['answer'].tolist()\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs, max_new_tokens=3, num_beams=3)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        answer = post_process_answer(answer, question, q_type)\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"yes\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\ntest_df['question_type'] = test_df['question'].apply(lambda q: get_question_type(q)[0])\n\ntest_df['answer'] = test_df['answer'].astype(str).apply(normalize_text)\ntest_df['predicted_answer'] = test_df['predicted_answer'].astype(str).apply(normalize_text)\n\ntest_df['correct'] = test_df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n)\naccuracy_by_type = test_df.groupby('question_type').apply(\n    lambda df: df['correct'].mean()\n)\nprint(\"\\nAccuracy by question type:\")\nprint(accuracy_by_type)\n\nmismatches = test_df[~test_df['correct']].head(5)\nfor idx, row in mismatches.iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"Ground Truth: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(\"---\")\n\noutput_path = Path(\"vqa_test_predictions_blip3_compatible.csv\")\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# --- Evaluation Metrics ---\ntqdm.pandas()\ndf = pd.read_csv(output_path)\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\naccuracy = df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n).mean()\n\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport subprocess\n\ndef run(cmd):\n    print(f\"Running: {cmd}\")\n    subprocess.check_call(cmd, shell=True)\n\n# Uninstall incompatible versions\nrun(f\"{sys.executable} -m pip uninstall -y torch torchvision torchaudio torchao transformers\")\n\n# Install compatible torch and transformers\nrun(f\"{sys.executable} -m pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\")\nrun(f\"{sys.executable} -m pip install transformers==4.40.2\")\n\n# Install other dependencies\nrun(f\"{sys.executable} -m pip install bert-score rouge-score nltk pillow pandas tqdm scikit-learn\")\n\nprint(\"✅ All dependencies installed. Please RESTART the runtime/kernel, then run the next cell.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport subprocess\n\ndef run(cmd):\n    print(f\"Running: {cmd}\")\n    subprocess.check_call(cmd, shell=True)\n\nrun(f\"{sys.executable} -m pip uninstall -y peft\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"run(f\"{sys.executable} -m pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\")\nrun(f\"{sys.executable} -m pip install transformers==4.40.2\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install peft --upgrade\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nimport subprocess\n\ndef run(cmd):\n    print(f\"Running: {cmd}\")\n    subprocess.check_call(cmd, shell=True)\n\n# Uninstall incompatible versions\nrun(f\"{sys.executable} -m pip uninstall -y torch torchvision torchaudio torchao transformers\")\n\n# Install compatible torch and transformers\nrun(f\"{sys.executable} -m pip install torch==2.2.2 torchvision==0.17.2 torchaudio==2.2.2 --index-url https://download.pytorch.org/whl/cu121\")\nrun(f\"{sys.executable} -m pip install transformers==4.40.2\")\n\n# Install other dependencies\nrun(f\"{sys.executable} -m pip install bert-score rouge-score nltk pillow pandas tqdm scikit-learn\")\n\nprint(\"✅ All dependencies installed. Please RESTART the runtime/kernel, then run the next cell.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration, Trainer, TrainingArguments\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom sklearn.metrics import f1_score\nfrom bert_score import score as bert_score\nfrom rouge_score import rouge_scorer\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# --- Device Setup ---\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- Hugging Face Token ---\nhf_token = \"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\"  # Replace with your token\n\n# --- Load Model and Processor ---\ntry:\n    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\", token=hf_token)\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\nexcept torch.cuda.OutOfMemoryError:\n    print(\"CUDA OOM, switching to CPU...\")\n    device = torch.device(\"cpu\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-flan-t5-xl\", torch_dtype=torch.float16, token=hf_token\n    ).to(device)\n\n# --- Load Data ---\ntry:\n    train_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_train.csv\")\n    test_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\nexcept FileNotFoundError as e:\n    print(f\"Dataset not found: {e}\")\n    raise\n\n# --- Common Answers and Synonyms ---\ncommon_answers = {\n    \"colors\": [\"white\", \"black\", \"blue\", \"red\", \"green\", \"yellow\", \"pink\", \"purple\", \"orange\", \"grey\", \"gray\", \"brown\", \"beige\", \"tan\", \"silver\", \"gold\", \"crimson\", \"navy\", \"violet\"],\n    \"materials\": [\"plastic\", \"silicone\", \"metal\", \"wood\", \"leather\", \"suede\", \"cotton\", \"polyester\", \"fabric\", \"synthetic\", \"brass\", \"steel\", \"canvas\", \"cloth\", \"rubber\"],\n    \"yes_no\": [\"yes\", \"no\"],\n    \"numbers\": [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\"],\n    \"phone_models\": [\"iphone\", \"samsung\", \"vivo\", \"oppo\", \"motorola\", \"lenovo\", \"redmi\", \"note\", \"moto\", \"lg\", \"nokia\"],\n    \"styles\": [\"modern\", \"floral\", \"geometric\", \"abstract\", \"minimalist\", \"rustic\", \"classic\", \"ankle\", \"block\", \"sling\", \"vintage\"],\n    \"objects\": [\"heart\", \"skull\", \"flowers\", \"moon\", \"stars\", \"cat\", \"dog\", \"lion\", \"butterflies\", \"teddy\", \"tree\"]\n}\nvalid_answers = set()\nfor category in common_answers.values():\n    valid_answers.update(category)\nsynonym_map = {\n    \"grey\": \"gray\", \"tan\": \"beige\", \"cloth\": \"fabric\", \"crimson\": \"red\", \"navy\": \"blue\", \"violet\": \"purple\"\n}\n\n# --- Helper Functions ---\ndef get_question_type(question):\n    question = question.lower()\n    if \"color\" in question or \"colour\" in question:\n        return \"color\", \"Answer with a color (e.g., red, blue, green): \"\n    elif \"material\" in question:\n        return \"material\", \"Answer with a material (e.g., plastic, leather, metal): \"\n    elif \"yes or no\" in question or \"is \" in question or \"are \" in question:\n        return \"yes_no\", \"Answer with yes or no: \"\n    elif \"how many\" in question:\n        return \"number\", \"Answer with a number (e.g., one, two, three): \"\n    elif \"phone\" in question or \"model\" in question:\n        return \"phone_model\", \"Answer with a phone model (e.g., iPhone, Samsung): \"\n    elif \"style\" in question or \"pattern\" in question:\n        return \"style\", \"Answer with a style or pattern (e.g., floral, modern): \"\n    else:\n        return \"object\", \"Answer with an object (e.g., heart, skull): \"\n\ndef post_process_answer(answer, question, q_type):\n    answer = answer.strip().lower()\n    answer_words = answer.split()\n    answer = answer_words[0] if answer_words else answer\n    answer = synonym_map.get(answer, answer)\n    if q_type == \"yes_no\":\n        if \"yes\" in answer:\n            return \"yes\"\n        if \"no\" in answer:\n            return \"no\"\n        return \"yes\"\n    elif q_type == \"color\":\n        if answer in common_answers[\"colors\"]:\n            return answer\n        return \"black\"\n    elif q_type == \"material\":\n        if answer in common_answers[\"materials\"]:\n            return answer\n        return \"plastic\"\n    elif q_type == \"number\":\n        if answer in common_answers[\"numbers\"]:\n            return answer\n        return \"two\"\n    elif q_type == \"phone_model\":\n        if answer in common_answers[\"phone_models\"]:\n            return answer\n        return \"iphone\"\n    elif q_type == \"style\":\n        if answer in common_answers[\"styles\"]:\n            return answer\n        return \"modern\"\n    elif q_type == \"object\":\n        if answer in common_answers[\"objects\"]:\n            return answer\n        return \"heart\"\n    if answer in valid_answers:\n        return answer\n    return \"yes\"\n\ndef normalize_text(s):\n    s = str(s).strip().lower()\n    s = re.sub(r'[^\\w\\s]', '', s)\n    s = re.sub(r'\\s+', ' ', s)\n    return s\n\n# --- Custom Dataset ---\nclass VQADataset(torch.utils.data.Dataset):\n    def __init__(self, df, processor):\n        self.df = df\n        self.processor = processor\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        image_path = row['full_path']\n        question = row['question']\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        answer = row['answer']\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except Exception as e:\n            print(f\"Failed to load image {image_path}: {e}\")\n            image = Image.new('RGB', (224, 224), color='gray')\n        inputs = self.processor(images=image, text=full_prompt, return_tensors=\"pt\", padding=True)\n        labels = self.processor.tokenizer(answer, return_tensors=\"pt\", padding=True, truncation=True).input_ids\n        for key in inputs:\n            inputs[key] = inputs[key].squeeze(0)\n        labels = labels.squeeze(0)\n        return {\n            \"pixel_values\": inputs[\"pixel_values\"], \n            \"input_ids\": inputs[\"input_ids\"], \n            \"attention_mask\": inputs[\"attention_mask\"], \n            \"labels\": labels\n        }\n\n# --- Data Split ---\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\ntrain_dataset = VQADataset(train_df, processor)\nval_dataset = VQADataset(val_df, processor)\n\n# --- Training Arguments ---\nsteps_per_epoch = len(train_dataset) // 2  # Batch size is 2\ntraining_args = TrainingArguments(\n    output_dir=\"./blip2_finetuned\",\n    num_train_epochs=3,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=2,\n    warmup_steps=50,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n    save_steps=500,\n    save_total_limit=1,\n    fp16=True,\n    evaluation_strategy=\"epoch\",   # transformers 4.40.2 uses 'evaluation_strategy'\n    remove_unused_columns=False\n)\n\n# --- Trainer ---\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n)\n\nprint(\"Starting fine-tuning...\")\ntrainer.train()\nprint(\"Fine-tuning completed.\")\n\n# --- Inference on Test Set ---\npredictions = []\nreferences = test_df['answer'].tolist()\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        q_type, prompt = get_question_type(question)\n        full_prompt = prompt + question\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=full_prompt, return_tensors=\"pt\").to(device, torch.float16)\n        output = model.generate(**inputs, max_new_tokens=3, num_beams=3)\n        answer = processor.decode(output[0], skip_special_tokens=True)\n        answer = post_process_answer(answer, question, q_type)\n        torch.cuda.empty_cache()\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"yes\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\ntest_df['question_type'] = test_df['question'].apply(lambda q: get_question_type(q)[0])\n\ntest_df['answer'] = test_df['answer'].astype(str).apply(normalize_text)\ntest_df['predicted_answer'] = test_df['predicted_answer'].astype(str).apply(normalize_text)\n\ntest_df['correct'] = test_df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n)\naccuracy_by_type = test_df.groupby('question_type').apply(\n    lambda df: df['correct'].mean()\n)\nprint(\"\\nAccuracy by question type:\")\nprint(accuracy_by_type)\n\nmismatches = test_df[~test_df['correct']].head(5)\nfor idx, row in mismatches.iterrows():\n    print(f\"Question: {row['question']}\")\n    print(f\"Ground Truth: {row['answer']}\")\n    print(f\"Predicted: {row['predicted_answer']}\")\n    print(\"---\")\n\noutput_path = Path(\"vqa_test_predictions_blip3_compatible.csv\")\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n\n# --- Evaluation Metrics ---\ntqdm.pandas()\ndf = pd.read_csv(output_path)\ndf['answer'] = df['answer'].astype(str).apply(normalize_text)\ndf['predicted_answer'] = df['predicted_answer'].astype(str).apply(normalize_text)\n\ndef compute_token_f1(pred, gt):\n    pred_tokens = pred.split()\n    gt_tokens = gt.split()\n    common = set(pred_tokens) & set(gt_tokens)\n    if not pred_tokens or not gt_tokens:\n        return 0.0\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(gt_tokens)\n    if precision + recall == 0:\n        return 0.0\n    return 2 * precision * recall / (precision + recall)\n\ndf['token_f1'] = df.progress_apply(lambda row: compute_token_f1(row['predicted_answer'], row['answer']), axis=1)\navg_token_f1 = df['token_f1'].mean()\naccuracy = df.apply(\n    lambda row: row['predicted_answer'] in row['answer'].split(),\n    axis=1\n).mean()\n\nrouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\ndf['rougeL'] = df.progress_apply(lambda row: rouge.score(row['answer'], row['predicted_answer'])['rougeL'].fmeasure, axis=1)\navg_rougeL = df['rougeL'].mean()\n\nP, R, F1 = bert_score(df['predicted_answer'].tolist(), df['answer'].tolist(), lang='en', verbose=True)\nbert_f1 = F1.mean().item()\n\nsmooth_fn = SmoothingFunction().method1\ndf['bleu'] = df.progress_apply(\n    lambda row: sentence_bleu([row['answer'].split()], row['predicted_answer'].split(), smoothing_function=smooth_fn),\n    axis=1\n)\navg_bleu = df['bleu'].mean()\n\nprint(f\"Token F1      : {avg_token_f1:.4f}\")\nprint(f\"Accuracy      : {accuracy:.4f}\")\nprint(f\"ROUGE-L       : {avg_rougeL:.4f}\")\nprint(f\"BERTScore F1  : {bert_f1:.4f}\")\nprint(f\"BLEU          : {avg_bleu:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nshutil.rmtree(\"/root/.cache/huggingface/hub/models--Salesforce--blip2-opt-2.7b\", ignore_errors=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport csv\nfrom pathlib import Path\nimport shutil\n\n# Clear any potentially corrupted cached model files\nshutil.rmtree(\"/root/.cache/huggingface/hub/models--Salesforce--blip2-opt-2.7b\", ignore_errors=True)\n\n# Output CSV\noutput_path = Path(\"vqa_test_predictions.csv\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (keep secret in production)\nhf_token = \"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\"\n\n# Load the processor and model with trust_remote_code\nprocessor = Blip2Processor.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    token=hf_token,\n    trust_remote_code=True\n)\n\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    token=hf_token,\n    trust_remote_code=True\n)\nmodel.eval()\n\n# Load data\ntrain_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\n\n# Run model on test set\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        ground_truth = row['answer']\n\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n        generated_ids = model.generate(**inputs, max_new_tokens=50)\n        answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\ntest_df['predicted_answer'] = predictions\ntest_df['correct'] = test_df['predicted_answer'].str.strip().str.lower() == test_df['answer'].str.strip().str.lower()\n\n# Save to CSV\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport shutil\nimport os\n\n# Force delete corrupted cache (for Kaggle/Colab only)\nmodel_cache_path = os.path.expanduser(\"~/.cache/huggingface/hub/models--Salesforce--blip2-opt-2.7b\")\nif os.path.exists(model_cache_path):\n    shutil.rmtree(model_cache_path, ignore_errors=True)\n\n# Output CSV path\noutput_path = Path(\"vqa_test_predictions.csv\")\n\n# Choose device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (use your own, keep private)\nhf_token = \"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\"\n\n# Load processor (DO NOT use trust_remote_code here)\nprocessor = Blip2Processor.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    token=hf_token\n)\n\n# Load model (trust_remote_code is required here)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    \"Salesforce/blip2-opt-2.7b\",\n    device_map=\"auto\",\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    token=hf_token,\n    trust_remote_code=True\n)\nmodel.eval()\n\n# Load test dataset\ntest_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\n\n# Predict answers\npredictions = []\nreferences = test_df['answer'].tolist()\n\nfor idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA on Test Set\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        ground_truth = row['answer']\n\n        # Load image and run processor\n        image = Image.open(image_path).convert(\"RGB\")\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n        inputs = {k: v.to(dtype=torch.float16 if torch.cuda.is_available() else torch.float32) for k, v in inputs.items()}\n\n        # Generate prediction\n        generated_ids = model.generate(**inputs, max_new_tokens=50)\n        answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    except Exception as e:\n        print(f\"Failed on {row['full_path']}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\n# Save results\ntest_df['predicted_answer'] = predictions\ntest_df['correct'] = test_df['predicted_answer'].str.strip().str.lower() == test_df['answer'].str.strip().str.lower()\ntest_df.to_csv(output_path, index=False)\n\nprint(f\"\\n✅ Predictions saved to: {output_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers==4.36.2 tokenizers==0.14.1 huggingface-hub>=0.19.3 --force-reinstall\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport shutil\n\n# Output file path\noutput_path = Path(\"vqa_test_predictions.csv\")\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hugging Face token (replace with your actual token)\nhf_token = \"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\"\n\n# Clear the local cache to avoid corrupt downloads\nmodel_id = \"Salesforce/blip2-opt-2.7b\"\ncache_path = os.path.expanduser(f\"~/.cache/huggingface/hub/models--{model_id.replace('/', '--')}\")\nif os.path.exists(cache_path):\n    shutil.rmtree(cache_path)\n\n# Load processor and model\nprocessor = Blip2Processor.from_pretrained(\n    model_id,\n    token=hf_token,\n    force_download=True\n)\nmodel = Blip2ForConditionalGeneration.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.float16,\n    token=hf_token,\n    force_download=True\n).to(device)\nmodel.eval()\n\n# Load dataset\ntrain_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vr4567/more_and_12049_test.csv\")\n\n# Predict answers\npredictions = []\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA\"):\n    try:\n        image_path = row['full_path']\n        question = row['question']\n        image = Image.open(image_path).convert(\"RGB\")\n\n        # Preprocess\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device, torch.float16)\n\n        # Generate answer\n        generated_ids = model.generate(**inputs, max_new_tokens=50)\n        answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    except Exception as e:\n        print(f\"Failed on {image_path}: {e}\")\n        answer = \"\"\n    predictions.append(answer)\n\n# Save results\ntest_df['predicted_answer'] = predictions\ntest_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\ntest_df.to_csv(output_path, index=False)\nprint(f\"Predictions saved to {output_path}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport logging\nimport warnings\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths\n# Change these paths to match your dataset location\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"\nOUTPUT_PATH = Path(\"vqa_test_predictions.csv\")\n\n# Set device - use MPS for Mac M1/M2 if available\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model(model_retry=0):\n    \"\"\"Load BLIP-2 model with retry logic\"\"\"\n    model_id = \"Salesforce/blip2-opt-2.7b\"\n    \n    try:\n        logger.info(f\"Loading model {model_id}, attempt {model_retry + 1}\")\n        \n        # Clear the cache for this model before loading to avoid corrupt files\n        cache_path = os.path.expanduser(f\"~/.cache/huggingface/hub/models--{model_id.replace('/', '--')}\")\n        if os.path.exists(cache_path):\n            import shutil\n            shutil.rmtree(cache_path)\n            logger.info(f\"Cleared cache at {cache_path}\")\n        \n        # First try without the token\n        try:\n            processor = Blip2Processor.from_pretrained(model_id)\n            model = Blip2ForConditionalGeneration.from_pretrained(\n                model_id,\n                device_map=\"auto\" if device.type == \"cuda\" else None,\n                torch_dtype=torch.float16 if device.type in [\"cuda\", \"mps\"] else torch.float32\n            ).to(device)\n            logger.info(\"Successfully loaded model without token\")\n        except Exception as no_token_error:\n            logger.warning(f\"Failed to load without token: {no_token_error}\")\n            logger.info(\"Trying with token...\")\n            \n            # If you have a Hugging Face token, use it (optional)\n            # Replace with your actual token if needed\n            hf_token = None  # Set to None to try without a token first\n            \n            processor = Blip2Processor.from_pretrained(\n                model_id,\n                token=hf_token\n            )\n            model = Blip2ForConditionalGeneration.from_pretrained(\n                model_id,\n                device_map=\"auto\" if device.type == \"cuda\" else None,\n                torch_dtype=torch.float16 if device.type in [\"cuda\", \"mps\"] else torch.float32,\n                token=hf_token\n            ).to(device)\n        \n        model.eval()\n        return processor, model\n    \n    except Exception as e:\n        if model_retry < 2:  # Try up to 3 times\n            logger.warning(f\"Failed to load model: {e}. Retrying...\")\n            return load_model(model_retry + 1)\n        else:\n            # If all attempts fail, try an alternative model\n            logger.warning(f\"Failed to load BLIP-2. Trying alternative model...\")\n            return load_alternative_model()\n\ndef load_alternative_model():\n    \"\"\"Fallback to a smaller BLIP-2 model if the main one fails\"\"\"\n    logger.info(\"Attempting to load smaller BLIP-2 model\")\n    \n    try:\n        from transformers import Blip2Processor, Blip2ForConditionalGeneration\n        \n        # Try a smaller BLIP-2 model\n        alt_model_id = \"Salesforce/blip2-opt-1.7b\"\n        \n        processor = Blip2Processor.from_pretrained(alt_model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            alt_model_id,\n            device_map=\"auto\" if device.type == \"cuda\" else None,\n            torch_dtype=torch.float16 if device.type in [\"cuda\", \"mps\"] else torch.float32\n        ).to(device)\n        \n        model.eval()\n        logger.info(\"Successfully loaded alternative BLIP-2 model\")\n        return processor, model\n    \n    except Exception as e:\n        logger.error(f\"Failed to load alternative model: {e}\")\n        raise RuntimeError(\"All model loading attempts failed\")\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        # Load dataset from configured paths\n        logger.info(f\"Loading training data from: {TRAIN_DATA_PATH}\")\n        train_df = pd.read_csv(TRAIN_DATA_PATH)\n        logger.info(f\"Loaded training data with {len(train_df)} samples\")\n        \n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        \n        # Validate expected columns exist\n        required_cols = ['full_path', 'question']\n        for col in required_cols:\n            if col not in test_df.columns:\n                raise ValueError(f\"Required column '{col}' not found in test data\")\n        \n        return test_df\n    except FileNotFoundError as e:\n        logger.error(f\"Data file not found: {e}\")\n        logger.error(\"Please check the TRAIN_DATA_PATH and TEST_DATA_PATH variables at the top of the script\")\n        raise\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Process a single VQA sample\"\"\"\n    try:\n        # Open and process image\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Prepare inputs\n        inputs = processor(images=image, text=question, return_tensors=\"pt\").to(device)\n        \n        # Generate answer\n        with torch.no_grad():\n            generated_ids = model.generate(**inputs, max_new_tokens=50)\n            answer = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n        \n        return answer\n    \n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return \"\"\n\ndef main():\n    try:\n        # Load model\n        processor, model = load_model()\n        \n        # Load dataset\n        test_df = process_data()\n        \n        # Predict answers\n        predictions = []\n        successes = 0\n        failures = 0\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                \n                answer = get_answer(processor, model, image_path, question)\n                predictions.append(answer)\n                successes += 1\n                \n                # Log progress periodically\n                if idx % 50 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples\")\n                \n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(\"\")\n                failures += 1\n        \n        # Save results\n        test_df['predicted_answer'] = predictions\n        \n        if 'answer' in test_df.columns:\n            # Calculate accuracy if answers are available\n            test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n            accuracy = test_df['correct'].mean()\n            logger.info(f\"Accuracy: {accuracy:.4f}\")\n        \n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n        \n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BlipProcessor, BlipForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport logging\nimport warnings\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths - IMPORTANT: Modify these to match your setup\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"  # Updated path\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"    # Updated path\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"  # Base directory for images\nOUTPUT_PATH = Path(\"vqa_test_predictions3.csv\")\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a simpler BLIP model that's more reliable\"\"\"\n    logger.info(\"Loading BLIP model\")\n    \n    try:\n        # Using the base BLIP model which is more reliable than BLIP-2\n        model_id = \"Salesforce/blip-vqa-base\"\n        \n        processor = BlipProcessor.from_pretrained(model_id)\n        model = BlipForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        \n        model.eval()\n        logger.info(\"Successfully loaded BLIP model\")\n        return processor, model\n    \n    except Exception as e:\n        logger.error(f\"Failed to load model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Check if the image exists at the given path, try alternatives if not\"\"\"\n    # First try the exact path provided\n    if os.path.exists(path):\n        return path\n    \n    # Try without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # Extract just the filename from the path\n    filename = os.path.basename(path)\n    \n    # Check if file exists in IMAGES_BASE_DIR\n    images_path = os.path.join(IMAGES_BASE_DIR, filename)\n    if os.path.exists(images_path):\n        return images_path\n    \n    # Try combining with various directory structures\n    image_dirs = [\n        IMAGES_BASE_DIR,                            # Direct images directory\n        \"\",                                         # Current directory\n        'images',                                   # Common subdir\n        'data/images',                              # Alternative subdir\n        '../images',                                # Parent dir\n        'input/images',                             # Another common structure\n        'vrmini2/abo-images-small/images',          # Explicit path\n        '/kaggle/input/vrmini2/abo-images-small/images'  # Full Kaggle path\n    ]\n    \n    # Try each directory\n    for img_dir in image_dirs:\n        alt_path = os.path.join(img_dir, filename)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # If still not found, extract potential subdirectories from the original path\n    # For example, if path is \"/kaggle/something/category/image.jpg\",\n    # try \"category/image.jpg\" in our base directories\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):  # Try up to 3 subdirectories\n        partial_path = os.path.join(*path_parts[-i:])\n        for img_dir in image_dirs:\n            alt_path = os.path.join(img_dir, partial_path)\n            if os.path.exists(alt_path):\n                return alt_path\n    \n    # Log warning about missing image\n    logger.warning(f\"Could not find image at any location: {path}\")\n    logger.warning(f\"Tried IMAGES_BASE_DIR: {IMAGES_BASE_DIR}\")\n    \n    # Return original path even though it doesn't exist\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        # Skip training data, only load test data\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        \n        # Verify that required columns exist\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Print a sample of the paths to debug\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        \n        # Only process a small subset for initial testing\n        logger.info(\"Processing full dataset\")\n        return test_df\n        \n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Process a single VQA sample\"\"\"\n    try:\n        # Verify image path\n        verified_path = verify_image_path(image_path)\n        \n        # Check if file exists\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Image not found\"\n        \n        # Open and process image\n        image = Image.open(verified_path).convert(\"RGB\")\n        \n        # Prepare inputs for BLIP\n        inputs = processor(image, question, return_tensors=\"pt\").to(device)\n        \n        # Generate answer\n        with torch.no_grad():\n            outputs = model.generate(**inputs)\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n        \n        return answer.strip()\n    \n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return f\"Error: {str(e)[:50]}\"  # Include short error message in prediction\n\ndef main():\n    try:\n        # Load model - using simpler BLIP model which is more reliable\n        processor, model = load_model()\n        \n        # Load dataset\n        test_df = process_data()\n        \n        # Log image directory structure\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        \n        # Predict answers\n        predictions = []\n        successes = 0\n        failures = 0\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                \n                # Print some debug info for the first few samples\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                \n                answer = get_answer(processor, model, image_path, question)\n                \n                # Print answers for first few samples\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                \n                predictions.append(answer)\n                \n                if \"Error\" not in answer and \"not found\" not in answer:\n                    successes += 1\n                else:\n                    failures += 1\n                \n                # Log progress periodically\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                \n                # Small delay to prevent overloading\n                time.sleep(0.01)\n                \n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(f\"Error: {str(e)[:50]}\")\n                failures += 1\n        \n        # Save results\n        test_df['predicted_answer'] = predictions\n        \n        if 'answer' in test_df.columns:\n            # Calculate accuracy if answers are available\n            test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n            accuracy = test_df['correct'].mean()\n            logger.info(f\"Accuracy: {accuracy:.4f}\")\n        \n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n        \n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport logging\nimport warnings\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths - IMPORTANT: Modify these to match your setup\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"  # Updated path\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"    # Updated path\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"  # Base directory for images\nOUTPUT_PATH = Path(\"vqa_test_predictions4.csv\")\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    \n    try:\n        # Use BLIP-2 model\n        model_id = \"Salesforce/blip2-opt-2.7b\"  # BLIP-2 model with OPT backbone\n        \n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        \n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    \n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Check if the image exists at the given path, try alternatives if not\"\"\"\n    # First try the exact path provided\n    if os.path.exists(path):\n        return path\n    \n    # Try without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # Extract just the filename from the path\n    filename = os.path.basename(path)\n    \n    # Check if file exists in IMAGES_BASE_DIR\n    images_path = os.path.join(IMAGES_BASE_DIR, filename)\n    if os.path.exists(images_path):\n        return images_path\n    \n    # Try combining with various directory structures\n    image_dirs = [\n        IMAGES_BASE_DIR,                            # Direct images directory\n        \"\",                                         # Current directory\n        'images',                                   # Common subdir\n        'data/images',                              # Alternative subdir\n        '../images',                                # Parent dir\n        'input/images',                             # Another common structure\n        'vrmini2/abo-images-small/images',          # Explicit path\n        '/kaggle/input/vrmini2/abo-images-small/images'  # Full Kaggle path\n    ]\n    \n    # Try each directory\n    for img_dir in image_dirs:\n        alt_path = os.path.join(img_dir, filename)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # If still not found, extract potential subdirectories from the original path\n    # For example, if path is \"/kaggle/something/category/image.jpg\",\n    # try \"category/image.jpg\" in our base directories\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):  # Try up to 3 subdirectories\n        partial_path = os.path.join(*path_parts[-i:])\n        for img_dir in image_dirs:\n            alt_path = os.path.join(img_dir, partial_path)\n            if os.path.exists(alt_path):\n                return alt_path\n    \n    # Log warning about missing image\n    logger.warning(f\"Could not find image at any location: {path}\")\n    logger.warning(f\"Tried IMAGES_BASE_DIR: {IMAGES_BASE_DIR}\")\n    \n    # Return original path even though it doesn't exist\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        # Skip training data, only load test data\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        \n        # Verify that required columns exist\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Print a sample of the paths to debug\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        \n        logger.info(\"Processing full dataset\")\n        return test_df\n        \n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Process a single VQA sample using BLIP-2\"\"\"\n    try:\n        # Verify image path\n        verified_path = verify_image_path(image_path)\n        \n        # Check if file exists\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Image not found\"\n        \n        # Open and process image\n        image = Image.open(verified_path).convert(\"RGB\")\n        \n        # BLIP-2 specific processing\n        # Format with the VQA prompt\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            # Generate answer with BLIP-2\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                num_beams=5,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            \n            # Try to extract just the answer part (strip the prompt)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n        \n        # Check if answer is empty or just repeating the question\n        if not answer or answer.strip() == question.strip():\n            answer = \"Unable to determine from image\"\n        \n        return answer.strip()\n    \n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return f\"Error: {str(e)[:50]}\"  # Include short error message in prediction\n\ndef main():\n    try:\n        # Load BLIP-2 model\n        processor, model = load_model()\n        \n        # Load dataset\n        test_df = process_data()\n        \n        # Log image directory structure\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        \n        # Predict answers\n        predictions = []\n        successes = 0\n        failures = 0\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                \n                # Print some debug info for the first few samples\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                \n                answer = get_answer(processor, model, image_path, question)\n                \n                # Print answers for first few samples\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                \n                predictions.append(answer)\n                \n                if \"Error\" not in answer and \"not found\" not in answer:\n                    successes += 1\n                else:\n                    failures += 1\n                \n                # Log progress periodically\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                \n                # Small delay to prevent overloading\n                time.sleep(0.01)\n                \n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(f\"Error: {str(e)[:50]}\")\n                failures += 1\n        \n        # Save results\n        test_df['predicted_answer'] = predictions\n        \n        if 'answer' in test_df.columns:\n            # Calculate accuracy if answers are available\n            test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n            accuracy = test_df['correct'].mean()\n            logger.info(f\"Accuracy: {accuracy:.4f}\")\n        \n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n        \n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport logging\nimport warnings\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths - IMPORTANT: Modify these to match your setup\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"  # Updated path\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"    # Updated path\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"  # Base directory for images\nOUTPUT_PATH = Path(\"vqa_test_predictions5.csv\")\n\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    \n    try:\n        # Use BLIP-2 model\n        model_id = \"Salesforce/blip2-opt-2.7b\"  # BLIP-2 model with OPT backbone\n        \n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        \n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    \n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Check if the image exists at the given path, try alternatives if not\"\"\"\n    # First try the exact path provided\n    if os.path.exists(path):\n        return path\n    \n    # Try without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # Extract just the filename from the path\n    filename = os.path.basename(path)\n    \n    # Check if file exists in IMAGES_BASE_DIR\n    images_path = os.path.join(IMAGES_BASE_DIR, filename)\n    if os.path.exists(images_path):\n        return images_path\n    \n    # Try combining with various directory structures\n    image_dirs = [\n        IMAGES_BASE_DIR,                            # Direct images directory\n        \"\",                                         # Current directory\n        'images',                                   # Common subdir\n        'data/images',                              # Alternative subdir\n        '../images',                                # Parent dir\n        'input/images',                             # Another common structure\n        'vrmini2/abo-images-small/images',          # Explicit path\n        '/kaggle/input/vrmini2/abo-images-small/images'  # Full Kaggle path\n    ]\n    \n    # Try each directory\n    for img_dir in image_dirs:\n        alt_path = os.path.join(img_dir, filename)\n        if os.path.exists(alt_path):\n            return alt_path\n    \n    # If still not found, extract potential subdirectories from the original path\n    # For example, if path is \"/kaggle/something/category/image.jpg\",\n    # try \"category/image.jpg\" in our base directories\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):  # Try up to 3 subdirectories\n        partial_path = os.path.join(*path_parts[-i:])\n        for img_dir in image_dirs:\n            alt_path = os.path.join(img_dir, partial_path)\n            if os.path.exists(alt_path):\n                return alt_path\n    \n    # Log warning about missing image\n    logger.warning(f\"Could not find image at any location: {path}\")\n    logger.warning(f\"Tried IMAGES_BASE_DIR: {IMAGES_BASE_DIR}\")\n    \n    # Return original path even though it doesn't exist\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        # Skip training data, only load test data\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        \n        # Verify that required columns exist\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Print a sample of the paths to debug\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        \n        logger.info(\"Processing full dataset\")\n        return test_df\n        \n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\n# Open and process image\n        image = Image.open(verified_path).convert(\"RGB\")\n        \n        # BLIP-2 specific processing\n        # Format with the VQA prompt\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        \n        with torch.no_grad():\n            # Generate answer with BLIP-2\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=50,\n                num_beams=5,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            \n            # Try to extract just the answer part (strip the prompt)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n        \n        # Check if answer is empty or just repeating the question\n        if not answer or answer.strip() == question.strip():\n            answer = \"Unable to determine from image\"\n        \n        return answer.strip()\n    \n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return f\"Error: {str(e)[:50]}\"  # Include short error message in prediction\n\ndef main():\n    try:\n        # Load BLIP-2 model\n        processor, model = load_model()\n        \n        # Load dataset\n        test_df = process_data()\n        \n        # Log image directory structure\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        \n        # Predict answers\n        predictions = []\n        successes = 0\n        failures = 0\n        \n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                \n                # Print some debug info for the first few samples\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                \n                answer = get_answer(processor, model, image_path, question)\n                \n                # Print answers for first few samples\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                \n                predictions.append(answer)\n                \n                if \"Error\" not in answer and \"not found\" not in answer:\n                    successes += 1\n                else:\n                    failures += 1\n                \n                # Log progress periodically\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                \n                # Small delay to prevent overloading\n                time.sleep(0.01)\n                \n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(f\"Error: {str(e)[:50]}\")\n                failures += 1\n        \n        # Save results\n        test_df['predicted_answer'] = predictions\n        \n        if 'answer' in test_df.columns:\n            # Calculate accuracy if answers are available\n            test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n            accuracy = test_df['correct'].mean()\n            logger.info(f\"Accuracy: {accuracy:.4f}\")\n        \n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n        \n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport os\nimport logging\nimport warnings\nimport time\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"\nOUTPUT_PATH = Path(\"vqa_test_predictions6.csv\")\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    try:\n        model_id = \"Salesforce/blip2-opt-2.7b\"\n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Check if the image exists at the given path, try alternatives if not\"\"\"\n    if os.path.exists(path):\n        return path\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            return alt_path\n    filename = os.path.basename(path)\n    images_path = os.path.join(IMAGES_BASE_DIR, filename)\n    if os.path.exists(images_path):\n        return images_path\n    image_dirs = [\n        IMAGES_BASE_DIR,\n        \"\",\n        'images',\n        'data/images',\n        '../images',\n        'input/images',\n        'vrmini2/abo-images-small/images',\n        '/kaggle/input/vrmini2/abo-images-small/images'\n    ]\n    for img_dir in image_dirs:\n        alt_path = os.path.join(img_dir, filename)\n        if os.path.exists(alt_path):\n            return alt_path\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):\n        partial_path = os.path.join(*path_parts[-i:])\n        for img_dir in image_dirs:\n            alt_path = os.path.join(img_dir, partial_path)\n            if os.path.exists(alt_path):\n                return alt_path\n    logger.warning(f\"Could not find image at any location: {path}\")\n    logger.warning(f\"Tried IMAGES_BASE_DIR: {IMAGES_BASE_DIR}\")\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        logger.info(\"Processing full dataset\")\n        return test_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Generate a one-word answer using BLIP-2\"\"\"\n    try:\n        verified_path = verify_image_path(image_path)\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Missing\"\n        image = Image.open(verified_path).convert(\"RGB\")\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1,  # Limit to one token\n                num_beams=3,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n            # Extract first word\n            answer = answer.split()[0] if answer.strip() else \"Unknown\"\n            if not answer or answer.strip() == question.strip():\n                answer = \"Unknown\"\n            return answer.strip()\n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return \"Error\"\n\ndef main():\n    try:\n        processor, model = load_model()\n        test_df = process_data()\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        predictions = []\n        successes = 0\n        failures = 0\n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                answer = get_answer(processor, model, image_path, question)\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                predictions.append(answer)\n                if answer not in [\"Error\", \"Missing\", \"Unknown\"]:\n                    successes += 1\n                else:\n                    failures += 1\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                time.sleep(0.01)\n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(\"Error\")\n                failures += 1\n        test_df['predicted_answer'] = predictions\n        if 'answer' in test_df.columns:\n            test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n            accuracy = test_df['correct'].mean()\n            logger.info(f\"Accuracy: {accuracy:.4f}\")\n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow CUDA warnings\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport logging\nimport warnings\nimport time\nimport re\ntry:\n    from word2number import w2n\nexcept ImportError:\n    raise ImportError(\"Please install word2number: `pip install word2number`\")\nfrom bert_score import score\nfrom sklearn.metrics import precision_recall_f1_support\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"\nOUTPUT_PATH = Path(\"vqa_test_predictions5.csv\")\n\n# Image path cache\nimage_path_cache = {}\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    try:\n        model_id = \"Salesforce/blip2-opt-2.7b\"\n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Robustly verify and find the image path, with recursive search and case-insensitive matching\"\"\"\n    global image_path_cache\n    \n    # Check cache first\n    if path in image_path_cache:\n        return image_path_cache[path]\n    \n    # Original path\n    if os.path.exists(path):\n        image_path_cache[path] = path\n        return path\n    \n    logger.debug(f\"Attempting to find image for path: {path}\")\n    \n    # Extract filename and its lowercase version\n    filename = os.path.basename(path)\n    filename_lower = filename.lower()\n    \n    # Define search directories\n    search_dirs = [\n        IMAGES_BASE_DIR,\n        \"/kaggle/input/vrmini2/abo-images-small/images/small\",\n        \"/kaggle/input/vrmini2/abo-images-small/images\",\n        \"/kaggle/input/vrmini2\",\n        \"images\",\n        \"data/images\",\n        \"input/images\",\n        \"\",\n        \"../images\",\n        \"/kaggle/input/vrmini2/abo-images-small\"\n    ]\n    \n    # Try exact path without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            image_path_cache[path] = alt_path\n            logger.debug(f\"Found image at: {alt_path}\")\n            return alt_path\n    \n    # Recursive search in each directory\n    for search_dir in search_dirs:\n        if not os.path.exists(search_dir):\n            logger.debug(f\"Search directory does not exist: {search_dir}\")\n            continue\n        try:\n            # Use pathlib for recursive search\n            for p in Path(search_dir).rglob(filename):\n                if p.is_file():\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image at: {str(p)}\")\n                    return str(p)\n            # Case-insensitive search\n            for p in Path(search_dir).rglob(\"*\"):\n                if p.is_file() and p.name.lower() == filename_lower:\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image (case-insensitive) at: {str(p)}\")\n                    return str(p)\n        except Exception as e:\n            logger.debug(f\"Error searching in {search_dir}: {e}\")\n    \n    # Try partial path Futbolcomponents (e.g., last 1-3 subdirectories)\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):\n        partial_path = os.path.join(*path_parts[-i:])\n        for search_dir in search_dirs:\n            alt_path = os.path.join(search_dir, partial_path)\n            if os.path.exists(alt_path):\n                image_path_cache[path] = alt_path\n                logger.debug(f\"Found image at partial path: {alt_path}\")\n                return alt_path\n            # Case-insensitive check\n            try:\n                for p in Path(search_dir).rglob(os.path.basename(partial_path)):\n                    if p.is_file() and p.name.lower() == os.path.basename(partial_path).lower():\n                        image_path_cache[path] = str(p)\n                        logger.debug(f\"Found image at partial path (case-insensitive): {str(p)}\")\n                        return str(p)\n            except Exception as e:\n                logger.debug(f\"Error searching partial path in {search_dir}: {e}\")\n    \n    # Log failure and directory structure for debugging\n    logger.warning(f\"Could not find image: {path}\")\n    logger.warning(f\"Tried directories: {search_dirs}\")\n    if os.path.exists(IMAGES_BASE_DIR):\n        try:\n            logger.warning(f\"Contents of {IMAGES_BASE_DIR}:\")\n            for item in os.listdir(IMAGES_BASE_DIR)[:10]:  # Limit to first 10 for brevity\n                logger.warning(f\"  {item}\")\n        except Exception as e:\n            logger.warning(f\"Could not list {IMAGES_BASE_DIR}: {e}\")\n    \n    # Cache the original path as a fallback (will be marked as missing)\n    image_path_cache[path] = path\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        logger.info(\"Processing full dataset\")\n        return test_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef has_number(text):\n    \"\"\"Check if the text contains a number (digits or words)\"\"\"\n    if not isinstance(text, str):\n        return False\n    # Check for digits\n    if re.search(r'\\d+', text):\n        return True\n    # Check for number words\n    number_words = [\n        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n        'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen',\n        'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty',\n        'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'thousand'\n    ]\n    words = text.lower().split()\n    return any(word in number_words for word in words)\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Generate a one-word answer using BLIP-2\"\"\"\n    try:\n        verified_path = verify_image_path(image_path)\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Missing\"\n        image = Image.open(verified_path).convert(\"RGB\")\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1,\n                num_beams=3,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n            answer = answer.split()[0] if answer.strip() else \"Unknown\"\n            if not answer or answer.strip() == question.strip():\n                answer = \"Unknown\"\n            return answer.strip()\n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return \"Error\"\n\ndef compute_metrics(test_df):\n    \"\"\"Compute accuracy, precision, recall, F1, and BERTScore\"\"\"\n    if 'answer' not in test_df.columns:\n        logger.warning(\"Ground truth answers not available, skipping metric computation\")\n        return None\n    # Exact match metrics\n    test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n    accuracy = test_df['correct'].mean()\n    # Precision, Recall, F1 for binary classification (correct/incorrect)\n    y_true = test_df['correct'].astype(int)\n    y_pred = test_df['correct'].astype(int)  # Using same for exact match\n    precision, recall, f1, _ = precision_recall_f1_support(y_true, y_pred, average='binary', zero_division=0)\n    # BERTScore\n    preds = test_df['predicted_answer'].fillna(\"Unknown\").tolist()\n    refs = test_df['answer'].fillna(\"Unknown\").tolist()\n    P, R, F1 = score(preds, refs, lang=\"en\", verbose=False)\n    bert_scores = {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'bertscore_precision': bert_scores['precision'],\n        'bertscore_recall': bert_scores['recall'],\n        'bertscore_f1': bert_scores['f1']\n    }\n    return metrics\n\ndef main():\n    try:\n        processor, model = load_model()\n        test_df = process_data()\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        predictions = []\n        contains_numbers = []\n        successes = 0\n        failures = 0\n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                answer = get_answer(processor, model, image_path, question)\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                predictions.append(answer)\n                # Check for numbers in predicted or ground truth answer\n                ground_truth = row.get('answer', '')\n                has_num = has_number(answer) or (has_number(ground_truth) if ground_truth else False)\n                contains_numbers.append(has_num)\n                if answer not in [\"Error\", \"Missing\", \"Unknown\"]:\n                    successes += 1\n                else:\n                    failures += 1\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                time.sleep(0.01)\n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(\"Error\")\n                contains_numbers.append(False)\n                failures += 1\n        test_df['predicted_answer'] = predictions\n        test_df['contains_number'] = contains_numbers\n        # Compute and log metrics\n        metrics = compute_metrics(test_df)\n        if metrics:\n            logger.info(\"Evaluation Metrics:\")\n            logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n            logger.info(f\"Precision: {metrics['precision']:.4f}\")\n            logger.info(f\"Recall: {metrics['recall']:.4f}\")\n            logger.info(f\"F1-Score: {metrics['f1']:.4f}\")\n            logger.info(f\"BERTScore Precision: {metrics['bertscore_precision']:.4f}\")\n            logger.info(f\"BERTScore Recall: {metrics['bertscore_recall']:.4f}\")\n            logger.info(f\"BERTScore F1: {metrics['bertscore_f1']:.4f}\")\n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install word2number bert-score scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install scikit-learn==1.3.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install word2number bert-score scikit-learn==1.3.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y scikit-learn\n!pip install scikit-learn==1.3.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn\nfrom sklearn.metrics import precision_recall_f1_support\nprint(sklearn.__version__)  # Should print 1.3.0 or similar\nprint(precision_recall_f1_support)  # Should print function reference","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow CUDA warnings\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport logging\nimport warnings\nimport time\nimport re\ntry:\n    from word2number import w2n\nexcept ImportError:\n    raise ImportError(\"Please install word2number: `pip install word2number`\")\nfrom bert_score import score\nfrom sklearn.metrics import precision_recall_f1_support\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"\nOUTPUT_PATH = Path(\"vqa_test_predictions5.csv\")\n\n# Image path cache\nimage_path_cache = {}\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    try:\n        model_id = \"Salesforce/blip2-opt-2.7b\"\n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Robustly verify and find the image path, with recursive search and case-insensitive matching\"\"\"\n    global image_path_cache\n    \n    # Check cache first\n    if path in image_path_cache:\n        return image_path_cache[path]\n    \n    # Original path\n    if os.path.exists(path):\n        image_path_cache[path] = path\n        return path\n    \n    logger.debug(f\"Attempting to find image for path: {path}\")\n    \n    # Extract filename and its lowercase version\n    filename = os.path.basename(path)\n    filename_lower = filename.lower()\n    \n    # Define search directories\n    search_dirs = [\n        IMAGES_BASE_DIR,\n        \"/kaggle/input/vrmini2/abo-images-small/images/small\",\n        \"/kaggle/input/vrmini2/abo-images-small/images\",\n        \"/kaggle/input/vrmini2\",\n        \"images\",\n        \"data/images\",\n        \"input/images\",\n        \"\",\n        \"../images\",\n        \"/kaggle/input/vrmini2/abo-images-small\"\n    ]\n    \n    # Try exact path without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            image_path_cache[path] = alt_path\n            logger.debug(f\"Found image at: {alt_path}\")\n            return alt_path\n    \n    # Recursive search in each directory\n    for search_dir in search_dirs:\n        if not os.path.exists(search_dir):\n            logger.debug(f\"Search directory does not exist: {search_dir}\")\n            continue\n        try:\n            # Use pathlib for recursive search\n            for p in Path(search_dir).rglob(filename):\n                if p.is_file():\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image at: {str(p)}\")\n                    return str(p)\n            # Case-insensitive search\n            for p in Path(search_dir).rglob(\"*\"):\n                if p.is_file() and p.name.lower() == filename_lower:\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image (case-insensitive) at: {str(p)}\")\n                    return str(p)\n        except Exception as e:\n            logger.debug(f\"Error searching in {search_dir}: {e}\")\n    \n    # Try partial path Futbolcomponents (e.g., last 1-3 subdirectories)\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):\n        partial_path = os.path.join(*path_parts[-i:])\n        for search_dir in search_dirs:\n            alt_path = os.path.join(search_dir, partial_path)\n            if os.path.exists(alt_path):\n                image_path_cache[path] = alt_path\n                logger.debug(f\"Found image at partial path: {alt_path}\")\n                return alt_path\n            # Case-insensitive check\n            try:\n                for p in Path(search_dir).rglob(os.path.basename(partial_path)):\n                    if p.is_file() and p.name.lower() == os.path.basename(partial_path).lower():\n                        image_path_cache[path] = str(p)\n                        logger.debug(f\"Found image at partial path (case-insensitive): {str(p)}\")\n                        return str(p)\n            except Exception as e:\n                logger.debug(f\"Error searching partial path in {search_dir}: {e}\")\n    \n    # Log failure and directory structure for debugging\n    logger.warning(f\"Could not find image: {path}\")\n    logger.warning(f\"Tried directories: {search_dirs}\")\n    if os.path.exists(IMAGES_BASE_DIR):\n        try:\n            logger.warning(f\"Contents of {IMAGES_BASE_DIR}:\")\n            for item in os.listdir(IMAGES_BASE_DIR)[:10]:  # Limit to first 10 for brevity\n                logger.warning(f\"  {item}\")\n        except Exception as e:\n            logger.warning(f\"Could not list {IMAGES_BASE_DIR}: {e}\")\n    \n    # Cache the original path as a fallback (will be marked as missing)\n    image_path_cache[path] = path\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        logger.info(\"Processing full dataset\")\n        return test_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef has_number(text):\n    \"\"\"Check if the text contains a number (digits or words)\"\"\"\n    if not isinstance(text, str):\n        return False\n    # Check for digits\n    if re.search(r'\\d+', text):\n        return True\n    # Check for number words\n    number_words = [\n        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n        'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen',\n        'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty',\n        'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'thousand'\n    ]\n    words = text.lower().split()\n    return any(word in number_words for word in words)\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Generate a one-word answer using BLIP-2\"\"\"\n    try:\n        verified_path = verify_image_path(image_path)\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Missing\"\n        image = Image.open(verified_path).convert(\"RGB\")\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1,\n                num_beams=3,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n            answer = answer.split()[0] if answer.strip() else \"Unknown\"\n            if not answer or answer.strip() == question.strip():\n                answer = \"Unknown\"\n            return answer.strip()\n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return \"Error\"\n\ndef compute_metrics(test_df):\n    \"\"\"Compute accuracy, precision, recall, F1, and BERTScore\"\"\"\n    if 'answer' not in test_df.columns:\n        logger.warning(\"Ground truth answers not available, skipping metric computation\")\n        return None\n    # Exact match metrics\n    test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n    accuracy = test_df['correct'].mean()\n    # Precision, Recall, F1 for binary classification (correct/incorrect)\n    y_true = test_df['correct'].astype(int)\n    y_pred = test_df['correct'].astype(int)  # Using same for exact match\n    precision, recall, f1, _ = precision_recall_f1_support(y_true, y_pred, average='binary', zero_division=0)\n    # BERTScore\n    preds = test_df['predicted_answer'].fillna(\"Unknown\").tolist()\n    refs = test_df['answer'].fillna(\"Unknown\").tolist()\n    P, R, F1 = score(preds, refs, lang=\"en\", verbose=False)\n    bert_scores = {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'bertscore_precision': bert_scores['precision'],\n        'bertscore_recall': bert_scores['recall'],\n        'bertscore_f1': bert_scores['f1']\n    }\n    return metrics\n\ndef main():\n    try:\n        processor, model = load_model()\n        test_df = process_data()\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        predictions = []\n        contains_numbers = []\n        successes = 0\n        failures = 0\n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                answer = get_answer(processor, model, image_path, question)\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                predictions.append(answer)\n                # Check for numbers in predicted or ground truth answer\n                ground_truth = row.get('answer', '')\n                has_num = has_number(answer) or (has_number(ground_truth) if ground_truth else False)\n                contains_numbers.append(has_num)\n                if answer not in [\"Error\", \"Missing\", \"Unknown\"]:\n                    successes += 1\n                else:\n                    failures += 1\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                time.sleep(0.01)\n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(\"Error\")\n                contains_numbers.append(False)\n                failures += 1\n        test_df['predicted_answer'] = predictions\n        test_df['contains_number'] = contains_numbers\n        # Compute and log metrics\n        metrics = compute_metrics(test_df)\n        if metrics:\n            logger.info(\"Evaluation Metrics:\")\n            logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n            logger.info(f\"Precision: {metrics['precision']:.4f}\")\n            logger.info(f\"Recall: {metrics['recall']:.4f}\")\n            logger.info(f\"F1-Score: {metrics['f1']:.4f}\")\n            logger.info(f\"BERTScore Precision: {metrics['bertscore_precision']:.4f}\")\n            logger.info(f\"BERTScore Recall: {metrics['bertscore_recall']:.4f}\")\n            logger.info(f\"BERTScore F1: {metrics['bertscore_f1']:.4f}\")\n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -y scikit-learn\n!pip install scikit-learn==1.3.0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sklearn\nprint(sklearn.__version__)  # Should print 1.3.0\n!pip list | grep scikit-learn","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip cache purge","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install word2number bert-score numpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import word2number\nprint(\"word2number imported successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install word2number bert-score numpy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bert_score\nimport numpy\nprint(bert_score.__version__)  # Should print version, e.g., 0.3.13\nprint(numpy.__version__)  # Should print version, e.g., 1.26.4\nprint(\"word2number imported successfully\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T18:36:53.078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TensorFlow CUDA warnings\n\nfrom transformers import Blip2Processor, Blip2ForConditionalGeneration\nfrom PIL import Image\nimport torch\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nimport logging\nimport warnings\nimport time\nimport re\ntry:\n    from word2number import w2n\nexcept ImportError:\n    raise ImportError(\"Please install word2number: `pip install word2number`\")\nfrom bert_score import score\n#from sklearn.metrics import precision_recall_f1_support\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\n# Configure paths\nTRAIN_DATA_PATH = \"/kaggle/input/vr1234/train_split.csv\"\nTEST_DATA_PATH = \"/kaggle/input/vr1234/test_split.csv\"\nIMAGES_BASE_DIR = \"/kaggle/input/vrmini2/abo-images-small/images/small/\"\nOUTPUT_PATH = Path(\"vqa_test_predictions7.csv\")\n\n# Image path cache\nimage_path_cache = {}\n\n# Set device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    logger.info(\"Using MPS device\")\nelse:\n    device = torch.device(\"cpu\")\n    logger.info(\"Using CPU\")\n\ndef load_model():\n    \"\"\"Load a BLIP-2 model for VQA\"\"\"\n    logger.info(\"Loading BLIP-2 model\")\n    try:\n        model_id = \"Salesforce/blip2-opt-2.7b\"\n        processor = Blip2Processor.from_pretrained(model_id)\n        model = Blip2ForConditionalGeneration.from_pretrained(\n            model_id,\n            torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32\n        ).to(device)\n        model.eval()\n        logger.info(\"Successfully loaded BLIP-2 model\")\n        return processor, model\n    except Exception as e:\n        logger.error(f\"Failed to load BLIP-2 model: {e}\")\n        raise RuntimeError(\"Model loading failed\")\n\ndef verify_image_path(path):\n    \"\"\"Robustly verify and find the image path, with recursive search and case-insensitive matching\"\"\"\n    global image_path_cache\n    \n    # Check cache first\n    if path in image_path_cache:\n        return image_path_cache[path]\n    \n    # Original path\n    if os.path.exists(path):\n        image_path_cache[path] = path\n        return path\n    \n    logger.debug(f\"Attempting to find image for path: {path}\")\n    \n    # Extract filename and its lowercase version\n    filename = os.path.basename(path)\n    filename_lower = filename.lower()\n    \n    # Define search directories\n    search_dirs = [\n        IMAGES_BASE_DIR,\n        \"/kaggle/input/vrmini2/abo-images-small/images/small\",\n        \"/kaggle/input/vrmini2/abo-images-small/images\",\n        \"/kaggle/input/vrmini2\",\n        \"images\",\n        \"data/images\",\n        \"input/images\",\n        \"\",\n        \"../images\",\n        \"/kaggle/input/vrmini2/abo-images-small\"\n    ]\n    \n    # Try exact path without /kaggle prefix\n    if path.startswith('/kaggle'):\n        alt_path = path.replace('/kaggle', '', 1)\n        if os.path.exists(alt_path):\n            image_path_cache[path] = alt_path\n            logger.debug(f\"Found image at: {alt_path}\")\n            return alt_path\n    \n    # Recursive search in each directory\n    for search_dir in search_dirs:\n        if not os.path.exists(search_dir):\n            logger.debug(f\"Search directory does not exist: {search_dir}\")\n            continue\n        try:\n            # Use pathlib for recursive search\n            for p in Path(search_dir).rglob(filename):\n                if p.is_file():\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image at: {str(p)}\")\n                    return str(p)\n            # Case-insensitive search\n            for p in Path(search_dir).rglob(\"*\"):\n                if p.is_file() and p.name.lower() == filename_lower:\n                    image_path_cache[path] = str(p)\n                    logger.debug(f\"Found image (case-insensitive) at: {str(p)}\")\n                    return str(p)\n        except Exception as e:\n            logger.debug(f\"Error searching in {search_dir}: {e}\")\n    \n    # Try partial path Futbolcomponents (e.g., last 1-3 subdirectories)\n    path_parts = Path(path).parts\n    for i in range(1, min(4, len(path_parts))):\n        partial_path = os.path.join(*path_parts[-i:])\n        for search_dir in search_dirs:\n            alt_path = os.path.join(search_dir, partial_path)\n            if os.path.exists(alt_path):\n                image_path_cache[path] = alt_path\n                logger.debug(f\"Found image at partial path: {alt_path}\")\n                return alt_path\n            # Case-insensitive check\n            try:\n                for p in Path(search_dir).rglob(os.path.basename(partial_path)):\n                    if p.is_file() and p.name.lower() == os.path.basename(partial_path).lower():\n                        image_path_cache[path] = str(p)\n                        logger.debug(f\"Found image at partial path (case-insensitive): {str(p)}\")\n                        return str(p)\n            except Exception as e:\n                logger.debug(f\"Error searching partial path in {search_dir}: {e}\")\n    \n    # Log failure and directory structure for debugging\n    logger.warning(f\"Could not find image: {path}\")\n    logger.warning(f\"Tried directories: {search_dirs}\")\n    if os.path.exists(IMAGES_BASE_DIR):\n        try:\n            logger.warning(f\"Contents of {IMAGES_BASE_DIR}:\")\n            for item in os.listdir(IMAGES_BASE_DIR)[:10]:  # Limit to first 10 for brevity\n                logger.warning(f\"  {item}\")\n        except Exception as e:\n            logger.warning(f\"Could not list {IMAGES_BASE_DIR}: {e}\")\n    \n    # Cache the original path as a fallback (will be marked as missing)\n    image_path_cache[path] = path\n    return path\n\ndef process_data():\n    \"\"\"Load and process the dataset\"\"\"\n    try:\n        logger.info(f\"Loading test data from: {TEST_DATA_PATH}\")\n        test_df = pd.read_csv(TEST_DATA_PATH)\n        logger.info(f\"Loaded test data with {len(test_df)} samples\")\n        required_cols = ['full_path', 'question']\n        missing_cols = [col for col in required_cols if col not in test_df.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        logger.info(\"Sample paths from dataset:\")\n        for i, path in enumerate(test_df['full_path'].iloc[:5]):\n            logger.info(f\"Sample {i}: {path}\")\n        logger.info(\"Processing full dataset\")\n        return test_df\n    except Exception as e:\n        logger.error(f\"Error loading data: {e}\")\n        raise\n\ndef has_number(text):\n    \"\"\"Check if the text contains a number (digits or words)\"\"\"\n    if not isinstance(text, str):\n        return False\n    # Check for digits\n    if re.search(r'\\d+', text):\n        return True\n    # Check for number words\n    number_words = [\n        'zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n        'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen',\n        'seventeen', 'eighteen', 'nineteen', 'twenty', 'thirty', 'forty', 'fifty',\n        'sixty', 'seventy', 'eighty', 'ninety', 'hundred', 'thousand'\n    ]\n    words = text.lower().split()\n    return any(word in number_words for word in words)\n\ndef get_answer(processor, model, image_path, question):\n    \"\"\"Generate a one-word answer using BLIP-2\"\"\"\n    try:\n        verified_path = verify_image_path(image_path)\n        if not os.path.exists(verified_path):\n            logger.warning(f\"Image not found: {verified_path}\")\n            return \"Missing\"\n        image = Image.open(verified_path).convert(\"RGB\")\n        prompt = f\"Question: {question} Answer:\"\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=1,\n                num_beams=3,\n                early_stopping=True\n            )\n            answer = processor.decode(outputs[0], skip_special_tokens=True)\n            if prompt in answer:\n                answer = answer.split(prompt)[1].strip()\n            answer = answer.split()[0] if answer.strip() else \"Unknown\"\n            if not answer or answer.strip() == question.strip():\n                answer = \"Unknown\"\n            return answer.strip()\n    except Exception as e:\n        logger.error(f\"Error processing {image_path}: {e}\")\n        return \"Error\"\n\ndef compute_metrics(test_df):\n    \"\"\"Compute accuracy, precision, recall, F1, and BERTScore\"\"\"\n    if 'answer' not in test_df.columns:\n        logger.warning(\"Ground truth answers not available, skipping metric computation\")\n        return None\n    # Exact match metrics\n    test_df['correct'] = test_df['predicted_answer'].str.lower().str.strip() == test_df['answer'].str.lower().str.strip()\n    accuracy = test_df['correct'].mean()\n    # Precision, Recall, F1 for binary classification (correct/incorrect)\n    y_true = test_df['correct'].astype(int)\n    y_pred = test_df['correct'].astype(int)  # Using same for exact match\n    #precision, recall, f1, _ = precision_recall_f1_support(y_true, y_pred, average='binary', zero_division=0)\n    # BERTScore\n    preds = test_df['predicted_answer'].fillna(\"Unknown\").tolist()\n    refs = test_df['answer'].fillna(\"Unknown\").tolist()\n    P, R, F1 = score(preds, refs, lang=\"en\", verbose=False)\n    bert_scores = {\n        'precision': P.mean().item(),\n        'recall': R.mean().item(),\n        'f1': F1.mean().item()\n    }\n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1': f1,\n        'bertscore_precision': bert_scores['precision'],\n        'bertscore_recall': bert_scores['recall'],\n        'bertscore_f1': bert_scores['f1']\n    }\n    return metrics\n\ndef main():\n    try:\n        processor, model = load_model()\n        test_df = process_data()\n        logger.info(f\"Image base directory: {IMAGES_BASE_DIR}\")\n        if os.path.exists(IMAGES_BASE_DIR):\n            logger.info(f\"Image directory exists and contains {len(os.listdir(IMAGES_BASE_DIR))} files/directories\")\n        else:\n            logger.warning(f\"Image directory does not exist: {IMAGES_BASE_DIR}\")\n        predictions = []\n        contains_numbers = []\n        successes = 0\n        failures = 0\n        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Running VQA with BLIP-2\"):\n            try:\n                image_path = row['full_path']\n                question = row['question']\n                if idx < 5:\n                    logger.info(f\"Processing sample {idx}\")\n                    logger.info(f\"Original image path: {image_path}\")\n                    verified_path = verify_image_path(image_path)\n                    logger.info(f\"Verified image path: {verified_path}\")\n                    logger.info(f\"Image exists: {os.path.exists(verified_path)}\")\n                    logger.info(f\"Question: {question}\")\n                answer = get_answer(processor, model, image_path, question)\n                if idx < 5:\n                    logger.info(f\"Generated answer: {answer}\")\n                predictions.append(answer)\n                # Check for numbers in predicted or ground truth answer\n                ground_truth = row.get('answer', '')\n                has_num = has_number(answer) or (has_number(ground_truth) if ground_truth else False)\n                contains_numbers.append(has_num)\n                if answer not in [\"Error\", \"Missing\", \"Unknown\"]:\n                    successes += 1\n                else:\n                    failures += 1\n                if idx % 20 == 0 and idx > 0:\n                    logger.info(f\"Processed {idx}/{len(test_df)} samples - Success: {successes}, Failures: {failures}\")\n                time.sleep(0.01)\n            except Exception as e:\n                logger.error(f\"Failed on sample {idx}: {e}\")\n                predictions.append(\"Error\")\n                contains_numbers.append(False)\n                failures += 1\n        test_df['predicted_answer'] = predictions\n        test_df['contains_number'] = contains_numbers\n        # Compute and log metrics\n        metrics = compute_metrics(test_df)\n        if metrics:\n            logger.info(\"Evaluation Metrics:\")\n            logger.info(f\"Accuracy: {metrics['accuracy']:.4f}\")\n            logger.info(f\"Precision: {metrics['precision']:.4f}\")\n            logger.info(f\"Recall: {metrics['recall']:.4f}\")\n            logger.info(f\"F1-Score: {metrics['f1']:.4f}\")\n            logger.info(f\"BERTScore Precision: {metrics['bertscore_precision']:.4f}\")\n            logger.info(f\"BERTScore Recall: {metrics['bertscore_recall']:.4f}\")\n            logger.info(f\"BERTScore F1: {metrics['bertscore_f1']:.4f}\")\n        test_df.to_csv(OUTPUT_PATH, index=False)\n        logger.info(f\"Predictions saved to {OUTPUT_PATH}\")\n        logger.info(f\"Successfully processed: {successes}, Failed: {failures}\")\n    except Exception as e:\n        logger.error(f\"Error in main execution: {e}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-17T18:36:53.083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}