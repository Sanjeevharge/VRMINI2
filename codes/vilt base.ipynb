{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11803940,"sourceType":"datasetVersion","datasetId":7405059}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers bert-score torch torchvision --quiet\n\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\nfrom PIL import Image\nimport torch\nfrom bert_score import score as bert_score\nfrom tqdm import tqdm\nimport re\n\n\n# === Helper function for text normalization ===\ndef normalize_text(text):\n    \"\"\"Normalize text for consistent comparison\"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# === Load CSV ===\ndf = pd.read_csv('/kaggle/input/vrmini2/12049.csv')\n\n# === 80/20 Split ===\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\ntrain_df.to_csv(\"train_split.csv\", index=False)\ntest_df.to_csv(\"test_split.csv\", index=False)\n\n\n# === Load ViLT model ===\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# === Prediction function ===\ndef predict_answer(image_path, question):\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        encoding = processor(image, question, return_tensors=\"pt\").to(model.device)\n        with torch.no_grad():\n            outputs = model(**encoding)\n        predicted_idx = outputs.logits.argmax(-1).item()\n        return model.config.id2label[predicted_idx]\n    except Exception as e:\n        print(f\"Error with {image_path}: {e}\")\n        return \"unknown\"\n\n# === Run predictions ===\ntqdm.pandas()\ntest_df[\"prediction\"] = test_df.progress_apply(\n    lambda row: predict_answer(row[\"full_path\"], row[\"question\"]), axis=1\n)\n\n# === Normalize predictions and answers ===\ntest_df[\"norm_answer\"] = test_df[\"answer\"].apply(normalize_text)\ntest_df[\"norm_prediction\"] = test_df[\"prediction\"].apply(normalize_text)\n\n# === Compute Matches ===\ntest_df[\"match\"] = test_df[\"norm_answer\"] == test_df[\"norm_prediction\"]\n\n# === Metrics ===\naccuracy = accuracy_score(test_df[\"norm_answer\"], test_df[\"norm_prediction\"])\nf1 = f1_score(test_df[\"norm_answer\"], test_df[\"norm_prediction\"], average=\"macro\")\n\nP, R, F1_bert = bert_score(test_df[\"norm_prediction\"].tolist(), test_df[\"norm_answer\"].tolist(), lang=\"en\", rescale_with_baseline=True)\nbert_f1_mean = F1_bert.mean().item()\n\n# === Print Results ===\nprint(\"\\n=== Evaluation Metrics ===\")\nprint(f\"Accuracy     : {accuracy:.4f}\")\nprint(f\"F1 Score     : {f1:.4f}\")\nprint(f\"BERTScore F1 : {bert_f1_mean:.4f}\")\n\n# === Sample predictions ===\nprint(\"\\n=== Sample Predictions vs References ===\")\nsample = test_df.sample(5, random_state=42)\nfor _, row in sample.iterrows():\n    print(f\"Question : {row['question']}\")\n    print(f\"Reference: {row['answer']} (Normalized: {row['norm_answer']})\")\n    print(f\"Prediction: {row['prediction']} (Normalized: {row['norm_prediction']})\")\n    print(f\"Match    : {row['match']}\")\n    print(\"---\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:36:40.324749Z","iopub.execute_input":"2025-05-14T05:36:40.325765Z","iopub.status.idle":"2025-05-14T05:37:44.286760Z","shell.execute_reply.started":"2025-05-14T05:36:40.325733Z","shell.execute_reply":"2025-05-14T05:37:44.285818Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n100%|██████████| 2410/2410 [00:57<00:00, 42.08it/s]\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\n=== Evaluation Metrics ===\nAccuracy     : 0.2490\nF1 Score     : 0.0356\nBERTScore F1 : 0.6896\n\n=== Sample Predictions vs References ===\nQuestion : Considering the image and metadata, where would this blanket be most appropriately used?\nReference: Outdoors (Normalized: outdoors)\nPrediction: yes (Normalized: yes)\nMatch    : False\n---\nQuestion : Based on the image and metadata, where would this product most likely be found in a physical store?\nReference: Kitchen (Normalized: kitchen)\nPrediction: store (Normalized: store)\nMatch    : False\n---\nQuestion : What is the color of the shoe?\nReference: Grey (Normalized: grey)\nPrediction: brown (Normalized: brown)\nMatch    : False\n---\nQuestion : Considering the image and metadata, what is the primary design feature of this phone case?\nReference: Birds (Normalized: birds)\nPrediction: face (Normalized: face)\nMatch    : False\n---\nQuestion : Is this binder designed for heavy-duty use or everyday use?\nReference: Everyday (Normalized: everyday)\nPrediction: no (Normalized: no)\nMatch    : False\n---\n","output_type":"stream"}],"execution_count":2}]}