{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11804756,"sourceType":"datasetVersion","datasetId":7405059}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas numpy torch transformers peft scikit-learn Pillow tqdm\nimport pandas as pd\nimport numpy as np\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    ViltProcessor, \n    ViltForQuestionAnswering,\n    get_linear_schedule_with_warmup\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model\n)\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom PIL import Image\nimport re\nfrom tqdm import tqdm\n\n# === Helper function for text normalization ===\ndef normalize_text(text):\n    \"\"\"Normalize text for consistent comparison\"\"\"\n    if not isinstance(text, str):\n        text = str(text)\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# === Load the data ===\ntrain_df = pd.read_csv(\"/kaggle/input/vrmini2/train_split.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/vrmini2/test_split.csv\")\n\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"Testing samples: {len(test_df)}\")\n\n# === Device configuration ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# === Load the pre-trained model and processor ===\nprocessor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\nbase_model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-finetuned-vqa\")\n\n# Set max sequence length for text tokenization\nMAX_LENGTH = 40  # Adjust as needed based on your data\n\n# === IMPROVEMENT: Adding answer frequency analysis ===\nprint(\"Analyzing answer distribution in training data...\")\nanswer_counts = train_df['answer'].value_counts()\nprint(f\"Top 10 most common answers: {answer_counts.head(10)}\")\nprint(f\"Number of unique answers: {len(answer_counts)}\")\n\n# === Create a custom dataset - FIXED IMPLEMENTATION ===\nclass VQADataset(Dataset):\n    def __init__(self, dataframe, processor, max_length=40):\n        self.dataframe = dataframe\n        self.processor = processor\n        self.max_length = max_length\n        \n        # IMPROVEMENT: Better handling of answers\n        # First normalize all answers for consistency\n        self.dataframe['normalized_answer'] = self.dataframe['answer'].apply(normalize_text)\n        \n        # Get all unique normalized answers in the training set\n        all_answers = sorted(self.dataframe['normalized_answer'].unique())\n        self.answer_to_id = {answer: idx for idx, answer in enumerate(all_answers)}\n        self.id_to_answer = {idx: answer for answer, idx in self.answer_to_id.items()}\n        self.num_labels = len(self.answer_to_id)\n        \n        print(f\"Dataset created with {self.num_labels} unique normalized answers\")\n        \n    def __len__(self):\n        return len(self.dataframe)\n    \n    def __getitem__(self, idx):\n        row = self.dataframe.iloc[idx]\n        image_path = row['full_path']\n        question = row['question']\n        answer = row['normalized_answer']  # Use normalized answer\n        \n        try:\n            # Load image\n            image = Image.open(image_path).convert(\"RGB\")\n            \n            # Pre-resize image to ensure consistent dimensions\n            # ViLT typically uses 384x384 images\n            image = image.resize((384, 384))\n            \n            # Process with processor after consistent resizing\n            encoding = self.processor(\n                images=image, \n                text=question, \n                return_tensors=\"pt\", \n                padding=\"max_length\",\n                max_length=self.max_length,\n                truncation=True\n            )\n            \n            # Remove batch dimension\n            for k, v in encoding.items():\n                encoding[k] = v.squeeze()\n                \n            # Convert answer to id\n            answer_id = self.answer_to_id.get(answer, -1)\n            \n            # FIXED: Create a one-hot vector for the answer instead of scalar\n            return {\n                \"pixel_values\": encoding[\"pixel_values\"],\n                \"input_ids\": encoding[\"input_ids\"],\n                \"attention_mask\": encoding[\"attention_mask\"],\n                \"token_type_ids\": encoding[\"token_type_ids\"],\n                \"labels\": answer_id  # Still return scalar for efficient batching\n            }\n        except Exception as e:\n            print(f\"Error processing {image_path}: {e}\")\n            # Return a properly formatted dummy tensor instead of None\n            return self._get_dummy_item()\n    \n    # Method to create dummy items for error cases \n    def _get_dummy_item(self):\n        # Create a dummy item with the right shapes\n        dummy_image = torch.zeros((3, 384, 384), dtype=torch.float32)\n        dummy_ids = torch.zeros(self.max_length, dtype=torch.long)\n        dummy_mask = torch.zeros(self.max_length, dtype=torch.long)\n        dummy_type_ids = torch.zeros(self.max_length, dtype=torch.long)\n        dummy_label = 0  # Return scalar\n        \n        return {\n            \"pixel_values\": dummy_image,\n            \"input_ids\": dummy_ids,\n            \"attention_mask\": dummy_mask,\n            \"token_type_ids\": dummy_type_ids,\n            \"labels\": dummy_label\n        }\n\n# === IMPROVED: Better batch size balance ===\nBATCH_SIZE = 8\n\n# === FIXED: Updated collate function for VQA model ===\ndef collate_fn(batch):\n    # Filter out None values in case there are any\n    batch = [item for item in batch if item is not None]\n    if not batch:\n        raise ValueError(\"Empty batch after filtering None values\")\n    \n    # Verify all pixel_values have the same shape\n    shapes = [item[\"pixel_values\"].shape for item in batch]\n    if len(set(str(s) for s in shapes)) > 1:\n        print(f\"Warning: Inconsistent shapes found: {shapes}\")\n    \n    # Create a batch-sized tensor of label indices\n    label_indices = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n    \n    return {\n        \"pixel_values\": torch.stack([item[\"pixel_values\"] for item in batch]),\n        \"input_ids\": torch.stack([item[\"input_ids\"] for item in batch]),\n        \"attention_mask\": torch.stack([item[\"attention_mask\"] for item in batch]),\n        \"token_type_ids\": torch.stack([item[\"token_type_ids\"] for item in batch]),\n        \"label_indices\": label_indices  # Keep original indices for evaluation\n    }\n\n# === Create datasets and dataloaders ===\ntrain_dataset = VQADataset(train_df, processor, max_length=MAX_LENGTH)\ntrain_dataloader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    collate_fn=collate_fn,\n    num_workers=0\n)\n\n# === Model preparation ===\nnum_labels = len(train_dataset.answer_to_id)\nprint(f\"Number of unique answers: {num_labels}\")\n\n# FIXED: Properly configure the model for VQA\n# The ViLT VQA model expects to output num_classes predictions\nbase_model.config.num_labels = num_labels\nbase_model.classifier = nn.Sequential(\n    nn.Linear(base_model.config.hidden_size, base_model.config.hidden_size),\n    nn.LayerNorm(base_model.config.hidden_size),\n    nn.GELU(),\n    nn.Linear(base_model.config.hidden_size, num_labels)\n)\n\n# IMPROVEMENT: Better LoRA configuration\nlora_config = LoraConfig(\n    r=32,                      # Higher rank for more expressivity\n    lora_alpha=64,             # Higher alpha for better scaling\n    lora_dropout=0.05,         # Lower dropout to avoid underfitting\n    target_modules=[\"query\", \"key\", \"value\", \"output.dense\"],\n    bias=\"none\",               # Don't adapt bias terms\n    task_type=\"QUESTION_ANS\"   # Specify the task type\n)\n\n# Apply LoRA to the model\nmodel = get_peft_model(base_model, lora_config)\nmodel.to(device)\n\n# Print trainable parameter counts\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nall_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} out of {all_params:,} ({100 * trainable_params / all_params:.2f}%)\")\n\n# Ensure model's config is updated with our answer map\nmodel.config.id2label = train_dataset.id_to_answer\nmodel.config.label2id = train_dataset.answer_to_id\nmodel.config.num_labels = num_labels\n\n# === IMPROVEMENT: Better training parameters ===\nnum_epochs = 5              # More epochs for better learning\nlearning_rate = 5e-4        # Higher learning rate\nweight_decay = 0.01         # Weight decay for regularization\noptimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n\n# Better training scheduler\ntotal_steps = len(train_dataloader) * num_epochs\nwarmup_steps = int(0.1 * total_steps)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps\n)\n\n# === IMPROVED: Prediction function with consistent image sizing ===\ndef predict_answer_finetuned(image_path, question, model, processor, id2label, max_length, device):\n    \"\"\"Improved prediction function with consistent image sizing\"\"\"\n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # Always resize to 384x384 for consistency\n        image = image.resize((384, 384))\n        \n        encoding = processor(\n            images=image, \n            text=question, \n            return_tensors=\"pt\",\n            padding=\"max_length\",\n            max_length=max_length,\n            truncation=True\n        ).to(device)\n        \n        with torch.no_grad():\n            outputs = model(**encoding)\n        \n        # Get probabilities\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=-1)[0]\n        \n        # Get top prediction\n        predicted_idx = torch.argmax(probs).item()\n        confidence = probs[predicted_idx].item()\n        \n        predicted_answer = id2label.get(predicted_idx, \"unknown\")\n        \n        return predicted_answer\n    except Exception as e:\n        print(f\"Error with {image_path}: {e}\")\n        return \"unknown\"\n\n# Define a custom loss function for VQA\nclass VQALoss(nn.Module):\n    def __init__(self, num_labels):\n        super(VQALoss, self).__init__()\n        self.num_labels = num_labels\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n    def forward(self, logits, label_indices):\n        return self.loss_fn(logits, label_indices)\n\n# Initialize loss function\nvqa_loss_fn = VQALoss(num_labels).to(device)\n\n# === FIXED: Training loop ===\nprint(\"Starting training...\")\nbest_loss = float('inf')\n\nfor epoch in range(num_epochs):\n    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n    model.train()\n    running_loss = 0.0\n    \n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\")\n    for step, batch in enumerate(progress_bar):\n        try:\n            # Move tensors to device\n            pixel_values = batch[\"pixel_values\"].to(device)\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            token_type_ids = batch[\"token_type_ids\"].to(device)\n            label_indices = batch[\"label_indices\"].to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass - don't pass labels to model, will handle manually\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                token_type_ids=token_type_ids,\n                pixel_values=pixel_values\n            )\n            \n            # FIXED: Handle VQA model outputs properly\n            # Calculate loss using our custom loss function\n            loss = vqa_loss_fn(outputs.logits, label_indices)\n            \n            # Backward pass\n            loss.backward()\n            \n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            # Update parameters\n            optimizer.step()\n            scheduler.step()\n            \n            # Update progress bar\n            running_loss += loss.item()\n            progress_bar.set_postfix({\"loss\": running_loss / (step + 1)})\n            \n            # Learning rate monitoring\n            if step % 100 == 0:\n                print(f\"Step {step}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n                \n            # Save checkpoint for best loss\n            if (step + 1) % 200 == 0:\n                avg_loss = running_loss / (step + 1)\n                if avg_loss < best_loss:\n                    best_loss = avg_loss\n                    print(f\"Saving checkpoint with loss: {best_loss:.4f}\")\n                    checkpoint_path = f\"./vilt_lora_checkpoint_epoch{epoch+1}_step{step+1}\"\n                    model.save_pretrained(checkpoint_path)\n                    \n        except Exception as e:\n            print(f\"Error in training batch: {e}\")\n            import traceback\n            traceback.print_exc()  # Print full traceback for debugging\n            continue\n    \n    avg_loss = running_loss / len(train_dataloader)\n    print(f\"Average loss: {avg_loss:.4f}\")\n    \n    # Periodic evaluation\n    if (epoch + 1) % 2 == 0 or epoch == num_epochs - 1:\n        # Quick validation on a subset\n        model.eval()\n        val_subset = train_df.sample(min(100, len(train_df)), random_state=epoch)\n        correct = 0\n        total = 0\n        \n        print(\"Running quick validation...\")\n        for _, row in tqdm(val_subset.iterrows(), total=len(val_subset)):\n            try:\n                image_path = row[\"full_path\"]\n                question = row[\"question\"]\n                true_answer = normalize_text(row[\"answer\"])\n                \n                pred_answer = predict_answer_finetuned(image_path, question, model, processor, \n                                                      train_dataset.id_to_answer, MAX_LENGTH, device)\n                pred_answer = normalize_text(pred_answer)\n                \n                if pred_answer == true_answer:\n                    correct += 1\n                total += 1\n            except Exception as e:\n                print(f\"Error in validation: {e}\")\n                \n        val_accuracy = correct / total if total > 0 else 0\n        print(f\"Validation Accuracy: {val_accuracy:.4f} ({correct}/{total})\")\n\n# === Save the fine-tuned model ===\nmodel_save_path = \"./vilt_lora_finetuned_final\"\nmodel.save_pretrained(model_save_path)\nprocessor.save_pretrained(model_save_path)\nprint(f\"Model saved to {model_save_path}\")\n\n# === Evaluation ===\nprint(\"Starting evaluation...\")\nmodel.eval()\n\n# Create a test dataset\ntest_dataset = VQADataset(test_df, processor, max_length=MAX_LENGTH)\ntest_dataloader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    collate_fn=collate_fn,\n    num_workers=0\n)\n\n# Run predictions\npredictions = []\nground_truth = []\n\nprint(\"Running predictions on test set...\")\nfor _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n    try:\n        image_path = row[\"full_path\"]\n        question = row[\"question\"]\n        true_answer = normalize_text(row[\"answer\"])\n        \n        pred_answer = predict_answer_finetuned(\n            image_path, question, model, processor, \n            train_dataset.id_to_answer, MAX_LENGTH, device\n        )\n        pred_answer = normalize_text(pred_answer)\n        \n        predictions.append(pred_answer)\n        ground_truth.append(true_answer)\n    except Exception as e:\n        print(f\"Error with prediction: {e}\")\n        predictions.append(\"unknown\")\n        ground_truth.append(true_answer)\n\n# Add predictions to dataframe\ntest_df[\"norm_answer\"] = test_df[\"answer\"].apply(normalize_text)\ntest_df[\"prediction\"] = predictions\ntest_df[\"norm_prediction\"] = test_df[\"prediction\"].apply(normalize_text)\n\n# Compute Matches\ntest_df[\"match\"] = test_df[\"norm_answer\"] == test_df[\"norm_prediction\"]\n\n# Metrics\naccuracy = accuracy_score(test_df[\"norm_answer\"], test_df[\"norm_prediction\"])\nf1 = f1_score(test_df[\"norm_answer\"], test_df[\"norm_prediction\"], average=\"macro\", zero_division=0)\n\n# Print Results\nprint(\"\\n=== Evaluation Metrics for LoRA Fine-tuned ViLT ===\")\nprint(f\"Accuracy     : {accuracy:.4f}\")\nprint(f\"F1 Score     : {f1:.4f}\")\n\n# Error analysis\nprint(\"\\n=== Error Analysis ===\")\nfrom collections import Counter\nerror_cases = test_df[~test_df[\"match\"]]\nprint(f\"Total errors: {len(error_cases)}\")\n\ntop_predictions = Counter(error_cases[\"norm_prediction\"]).most_common(5)\nprint(f\"Top wrong predictions: {top_predictions}\")\n\n# Check the most common answer categories\nprint(\"\\n=== Common Reference Answer Categories ===\")\nanswer_cats = Counter(test_df[\"norm_answer\"]).most_common(10)\nprint(f\"Top reference answers: {answer_cats}\")\n\n# Sample predictions\nprint(\"\\n=== Sample Predictions vs References ===\")\nsample = test_df.sample(10, random_state=42)\nfor _, row in sample.iterrows():\n    print(f\"Question : {row['question']}\")\n    print(f\"Reference: {row['answer']} (Normalized: {row['norm_answer']})\")\n    print(f\"Prediction: {row['prediction']} (Normalized: {row['norm_prediction']})\")\n    print(f\"Match    : {row['match']}\")\n    print(\"---\")\n\n# Save results\ntest_df.to_csv(\"test_results_lora_finetuned_improved.csv\", index=False)\nprint(\"Results saved to test_results_lora_finetuned_improved.csv\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-14T11:59:48.327669Z","iopub.execute_input":"2025-05-14T11:59:48.328002Z","iopub.status.idle":"2025-05-14T12:34:21.002358Z","shell.execute_reply.started":"2025-05-14T11:59:48.327977Z","shell.execute_reply":"2025-05-14T12:34:21.001340Z"}},"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.26.1)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.13.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from peft) (0.18.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (1.1.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nTraining samples: 9639\nTesting samples: 2410\nUsing device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Analyzing answer distribution in training data...\nTop 10 most common answers: answer\nYes         680\nPlastic     380\nBlack       355\nTwo         295\nBlue        249\nPink        202\nThree       164\nAbstract    143\nSilicone    139\nWhite       134\nName: count, dtype: int64\nNumber of unique answers: 1765\nDataset created with 1736 unique normalized answers\nNumber of unique answers: 1736\nTrainable parameters: 3,833,856 out of 117,355,976 (3.27%)\nStarting training...\nEpoch 1/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:   0%|          | 1/1205 [00:00<08:09,  2.46it/s, loss=7.62]","output_type":"stream"},{"name":"stdout","text":"Step 0, LR: 8.31e-07\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:   8%|▊         | 101/1205 [00:35<06:03,  3.04it/s, loss=7.47]","output_type":"stream"},{"name":"stdout","text":"Step 100, LR: 8.39e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  17%|█▋        | 200/1205 [01:07<05:35,  3.00it/s, loss=7.31]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 7.3084\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  17%|█▋        | 201/1205 [01:07<05:31,  3.02it/s, loss=7.31]","output_type":"stream"},{"name":"stdout","text":"Step 200, LR: 1.67e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  25%|██▍       | 301/1205 [01:40<05:01,  2.99it/s, loss=7.11]","output_type":"stream"},{"name":"stdout","text":"Step 300, LR: 2.50e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  33%|███▎      | 400/1205 [02:13<04:34,  2.93it/s, loss=6.95]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 6.9459\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  33%|███▎      | 401/1205 [02:14<04:30,  2.97it/s, loss=6.95]","output_type":"stream"},{"name":"stdout","text":"Step 400, LR: 3.33e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  42%|████▏     | 501/1205 [02:47<03:51,  3.04it/s, loss=6.83]","output_type":"stream"},{"name":"stdout","text":"Step 500, LR: 4.16e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  50%|████▉     | 600/1205 [03:20<03:26,  2.93it/s, loss=6.73]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 6.7262\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  50%|████▉     | 601/1205 [03:20<03:22,  2.98it/s, loss=6.72]","output_type":"stream"},{"name":"stdout","text":"Step 600, LR: 4.99e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  58%|█████▊    | 701/1205 [03:53<02:46,  3.02it/s, loss=6.62]","output_type":"stream"},{"name":"stdout","text":"Step 700, LR: 4.91e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  66%|██████▋   | 800/1205 [04:26<02:23,  2.81it/s, loss=6.53]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 6.5282\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  66%|██████▋   | 801/1205 [04:26<02:22,  2.83it/s, loss=6.53]","output_type":"stream"},{"name":"stdout","text":"Step 800, LR: 4.82e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  75%|███████▍  | 901/1205 [04:59<01:39,  3.06it/s, loss=6.44]","output_type":"stream"},{"name":"stdout","text":"Step 900, LR: 4.72e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  83%|████████▎ | 1000/1205 [05:32<01:09,  2.94it/s, loss=6.37]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 6.3745\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  83%|████████▎ | 1001/1205 [05:32<01:08,  2.97it/s, loss=6.37]","output_type":"stream"},{"name":"stdout","text":"Step 1000, LR: 4.63e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  91%|█████████▏| 1101/1205 [06:05<00:34,  3.02it/s, loss=6.32]","output_type":"stream"},{"name":"stdout","text":"Step 1100, LR: 4.54e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|█████████▉| 1200/1205 [06:38<00:01,  2.98it/s, loss=6.26]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 6.2614\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|█████████▉| 1201/1205 [06:38<00:01,  2.98it/s, loss=6.26]","output_type":"stream"},{"name":"stdout","text":"Step 1200, LR: 4.45e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1: 100%|██████████| 1205/1205 [06:39<00:00,  3.01it/s, loss=6.26]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 6.2593\nEpoch 2/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:   0%|          | 1/1205 [00:00<06:29,  3.09it/s, loss=6.12]","output_type":"stream"},{"name":"stdout","text":"Step 0, LR: 4.44e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:   8%|▊         | 101/1205 [00:33<06:04,  3.03it/s, loss=5.6] ","output_type":"stream"},{"name":"stdout","text":"Step 100, LR: 4.35e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  17%|█▋        | 200/1205 [01:06<05:41,  2.94it/s, loss=5.53]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.5317\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  17%|█▋        | 201/1205 [01:06<05:37,  2.97it/s, loss=5.53]","output_type":"stream"},{"name":"stdout","text":"Step 200, LR: 4.26e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  25%|██▍       | 301/1205 [01:39<04:54,  3.07it/s, loss=5.48]","output_type":"stream"},{"name":"stdout","text":"Step 300, LR: 4.17e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  33%|███▎      | 400/1205 [02:11<04:32,  2.96it/s, loss=5.46]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.4568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  33%|███▎      | 401/1205 [02:11<04:28,  2.99it/s, loss=5.46]","output_type":"stream"},{"name":"stdout","text":"Step 400, LR: 4.07e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  42%|████▏     | 501/1205 [02:44<03:48,  3.09it/s, loss=5.4] ","output_type":"stream"},{"name":"stdout","text":"Step 500, LR: 3.98e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  50%|████▉     | 600/1205 [03:17<03:23,  2.98it/s, loss=5.4] ","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.3973\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  50%|████▉     | 601/1205 [03:17<03:21,  3.00it/s, loss=5.4]","output_type":"stream"},{"name":"stdout","text":"Step 600, LR: 3.89e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  58%|█████▊    | 701/1205 [03:50<02:45,  3.05it/s, loss=5.35]","output_type":"stream"},{"name":"stdout","text":"Step 700, LR: 3.80e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  66%|██████▋   | 800/1205 [04:22<02:16,  2.96it/s, loss=5.35]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.3470\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  66%|██████▋   | 801/1205 [04:23<02:15,  2.98it/s, loss=5.35]","output_type":"stream"},{"name":"stdout","text":"Step 800, LR: 3.71e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  75%|███████▍  | 901/1205 [04:56<01:39,  3.06it/s, loss=5.34]","output_type":"stream"},{"name":"stdout","text":"Step 900, LR: 3.61e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  83%|████████▎ | 1000/1205 [05:28<01:08,  2.99it/s, loss=5.31]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.3115\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  83%|████████▎ | 1001/1205 [05:28<01:07,  3.01it/s, loss=5.31]","output_type":"stream"},{"name":"stdout","text":"Step 1000, LR: 3.52e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2:  91%|█████████▏| 1101/1205 [06:01<00:33,  3.07it/s, loss=5.3] ","output_type":"stream"},{"name":"stdout","text":"Step 1100, LR: 3.43e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|█████████▉| 1200/1205 [06:34<00:01,  2.93it/s, loss=5.28]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 5.2822\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|█████████▉| 1201/1205 [06:34<00:01,  2.97it/s, loss=5.28]","output_type":"stream"},{"name":"stdout","text":"Step 1200, LR: 3.34e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2: 100%|██████████| 1205/1205 [06:35<00:00,  3.04it/s, loss=5.28]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 5.2813\nRunning quick validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:03<00:00, 31.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.3300 (33/100)\nEpoch 3/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:   0%|          | 1/1205 [00:00<06:31,  3.08it/s, loss=6.06]","output_type":"stream"},{"name":"stdout","text":"Step 0, LR: 3.33e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:   8%|▊         | 101/1205 [00:33<05:58,  3.08it/s, loss=4.99]","output_type":"stream"},{"name":"stdout","text":"Step 100, LR: 3.24e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  17%|█▋        | 200/1205 [01:05<05:36,  2.98it/s, loss=4.95]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.9538\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  17%|█▋        | 201/1205 [01:05<05:34,  3.00it/s, loss=4.96]","output_type":"stream"},{"name":"stdout","text":"Step 200, LR: 3.15e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  25%|██▍       | 301/1205 [01:38<04:53,  3.08it/s, loss=4.9] ","output_type":"stream"},{"name":"stdout","text":"Step 300, LR: 3.06e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  33%|███▎      | 400/1205 [02:11<04:31,  2.96it/s, loss=4.9] ","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.8963\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  33%|███▎      | 401/1205 [02:11<04:35,  2.92it/s, loss=4.89]","output_type":"stream"},{"name":"stdout","text":"Step 400, LR: 2.96e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  42%|████▏     | 501/1205 [02:44<03:56,  2.98it/s, loss=4.9] ","output_type":"stream"},{"name":"stdout","text":"Step 500, LR: 2.87e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  50%|████▉     | 600/1205 [03:16<03:24,  2.96it/s, loss=4.88]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.8849\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  50%|████▉     | 601/1205 [03:16<03:22,  2.98it/s, loss=4.89]","output_type":"stream"},{"name":"stdout","text":"Step 600, LR: 2.78e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  58%|█████▊    | 701/1205 [03:49<02:45,  3.05it/s, loss=4.9] ","output_type":"stream"},{"name":"stdout","text":"Step 700, LR: 2.69e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  66%|██████▋   | 801/1205 [04:22<02:11,  3.07it/s, loss=4.89]","output_type":"stream"},{"name":"stdout","text":"Step 800, LR: 2.59e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  75%|███████▍  | 901/1205 [04:54<01:41,  3.00it/s, loss=4.86]","output_type":"stream"},{"name":"stdout","text":"Step 900, LR: 2.50e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  83%|████████▎ | 1000/1205 [05:27<01:08,  2.99it/s, loss=4.88]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.8771\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  83%|████████▎ | 1001/1205 [05:27<01:07,  3.02it/s, loss=4.88]","output_type":"stream"},{"name":"stdout","text":"Step 1000, LR: 2.41e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3:  91%|█████████▏| 1101/1205 [06:00<00:33,  3.09it/s, loss=4.89]","output_type":"stream"},{"name":"stdout","text":"Step 1100, LR: 2.32e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|█████████▉| 1200/1205 [06:32<00:01,  2.94it/s, loss=4.88]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.8751\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|█████████▉| 1201/1205 [06:32<00:01,  2.98it/s, loss=4.88]","output_type":"stream"},{"name":"stdout","text":"Step 1200, LR: 2.23e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3: 100%|██████████| 1205/1205 [06:34<00:00,  3.06it/s, loss=4.88]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 4.8756\nEpoch 4/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:   0%|          | 1/1205 [00:00<06:31,  3.08it/s, loss=4.74]","output_type":"stream"},{"name":"stdout","text":"Step 0, LR: 2.22e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:   8%|▊         | 101/1205 [00:32<05:59,  3.07it/s, loss=4.73]","output_type":"stream"},{"name":"stdout","text":"Step 100, LR: 2.13e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  17%|█▋        | 200/1205 [01:05<05:36,  2.99it/s, loss=4.69]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.6939\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  17%|█▋        | 201/1205 [01:05<05:32,  3.02it/s, loss=4.69]","output_type":"stream"},{"name":"stdout","text":"Step 200, LR: 2.04e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  25%|██▍       | 301/1205 [01:38<04:52,  3.09it/s, loss=4.66]","output_type":"stream"},{"name":"stdout","text":"Step 300, LR: 1.94e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  33%|███▎      | 400/1205 [02:10<04:30,  2.98it/s, loss=4.66]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.6585\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  33%|███▎      | 401/1205 [02:10<04:27,  3.01it/s, loss=4.66]","output_type":"stream"},{"name":"stdout","text":"Step 400, LR: 1.85e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  42%|████▏     | 501/1205 [02:43<03:51,  3.05it/s, loss=4.63]","output_type":"stream"},{"name":"stdout","text":"Step 500, LR: 1.76e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  50%|████▉     | 600/1205 [03:16<03:22,  2.98it/s, loss=4.62]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.6169\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  50%|████▉     | 601/1205 [03:16<03:20,  3.01it/s, loss=4.61]","output_type":"stream"},{"name":"stdout","text":"Step 600, LR: 1.67e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  58%|█████▊    | 701/1205 [03:49<02:46,  3.03it/s, loss=4.59]","output_type":"stream"},{"name":"stdout","text":"Step 700, LR: 1.58e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  66%|██████▋   | 800/1205 [04:21<02:15,  3.00it/s, loss=4.58]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.5761\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  66%|██████▋   | 801/1205 [04:21<02:14,  3.00it/s, loss=4.58]","output_type":"stream"},{"name":"stdout","text":"Step 800, LR: 1.48e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  75%|███████▍  | 901/1205 [04:54<01:38,  3.09it/s, loss=4.56]","output_type":"stream"},{"name":"stdout","text":"Step 900, LR: 1.39e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  83%|████████▎ | 1000/1205 [05:26<01:08,  2.99it/s, loss=4.56]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.5606\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  83%|████████▎ | 1001/1205 [05:27<01:07,  3.02it/s, loss=4.56]","output_type":"stream"},{"name":"stdout","text":"Step 1000, LR: 1.30e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4:  91%|█████████▏| 1101/1205 [06:00<00:34,  3.00it/s, loss=4.56]","output_type":"stream"},{"name":"stdout","text":"Step 1100, LR: 1.21e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|█████████▉| 1200/1205 [06:32<00:01,  2.97it/s, loss=4.55]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.5535\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|█████████▉| 1201/1205 [06:32<00:01,  3.01it/s, loss=4.56]","output_type":"stream"},{"name":"stdout","text":"Step 1200, LR: 1.11e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4: 100%|██████████| 1205/1205 [06:33<00:00,  3.06it/s, loss=4.56]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 4.5551\nRunning quick validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:03<00:00, 31.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4100 (41/100)\nEpoch 5/5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:   0%|          | 1/1205 [00:00<06:42,  2.99it/s, loss=3.95]","output_type":"stream"},{"name":"stdout","text":"Step 0, LR: 1.11e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:   8%|▊         | 101/1205 [00:33<06:07,  3.01it/s, loss=4.32]","output_type":"stream"},{"name":"stdout","text":"Step 100, LR: 1.02e-04\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  17%|█▋        | 200/1205 [01:05<05:39,  2.96it/s, loss=4.38]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3838\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  17%|█▋        | 201/1205 [01:05<05:34,  3.00it/s, loss=4.38]","output_type":"stream"},{"name":"stdout","text":"Step 200, LR: 9.26e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  25%|██▍       | 301/1205 [01:38<04:55,  3.06it/s, loss=4.39]","output_type":"stream"},{"name":"stdout","text":"Step 300, LR: 8.33e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  33%|███▎      | 400/1205 [02:11<04:39,  2.88it/s, loss=4.37]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3734\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  33%|███▎      | 401/1205 [02:11<04:37,  2.89it/s, loss=4.38]","output_type":"stream"},{"name":"stdout","text":"Step 400, LR: 7.41e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  42%|████▏     | 501/1205 [02:44<03:51,  3.04it/s, loss=4.35]","output_type":"stream"},{"name":"stdout","text":"Step 500, LR: 6.49e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  50%|████▉     | 600/1205 [03:16<03:22,  2.98it/s, loss=4.33]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3302\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  50%|████▉     | 601/1205 [03:17<03:23,  2.97it/s, loss=4.33]","output_type":"stream"},{"name":"stdout","text":"Step 600, LR: 5.57e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  58%|█████▊    | 701/1205 [03:49<02:45,  3.05it/s, loss=4.33]","output_type":"stream"},{"name":"stdout","text":"Step 700, LR: 4.65e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  66%|██████▋   | 800/1205 [04:22<02:16,  2.97it/s, loss=4.32]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3240\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  66%|██████▋   | 801/1205 [04:22<02:17,  2.94it/s, loss=4.32]","output_type":"stream"},{"name":"stdout","text":"Step 800, LR: 3.72e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  75%|███████▍  | 901/1205 [04:55<01:39,  3.05it/s, loss=4.31]","output_type":"stream"},{"name":"stdout","text":"Step 900, LR: 2.80e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  83%|████████▎ | 1000/1205 [05:28<01:09,  2.97it/s, loss=4.31]","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  83%|████████▎ | 1001/1205 [05:28<01:08,  2.98it/s, loss=4.31]","output_type":"stream"},{"name":"stdout","text":"Step 1000, LR: 1.88e-05\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5:  91%|█████████▏| 1101/1205 [06:01<00:33,  3.08it/s, loss=4.31]","output_type":"stream"},{"name":"stdout","text":"Step 1100, LR: 9.59e-06\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|█████████▉| 1200/1205 [06:34<00:01,  2.99it/s, loss=4.3] ","output_type":"stream"},{"name":"stdout","text":"Saving checkpoint with loss: 4.3044\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|█████████▉| 1201/1205 [06:34<00:01,  3.02it/s, loss=4.3]","output_type":"stream"},{"name":"stdout","text":"Step 1200, LR: 3.69e-07\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5: 100%|██████████| 1205/1205 [06:35<00:00,  3.04it/s, loss=4.3]\n","output_type":"stream"},{"name":"stdout","text":"Average loss: 4.3018\nRunning quick validation...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 100/100 [00:03<00:00, 31.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Accuracy: 0.4900 (49/100)\nModel saved to ./vilt_lora_finetuned_final\nStarting evaluation...\nDataset created with 738 unique normalized answers\nRunning predictions on test set...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2410/2410 [01:17<00:00, 31.04it/s]","output_type":"stream"},{"name":"stdout","text":"\n=== Evaluation Metrics for LoRA Fine-tuned ViLT ===\nAccuracy     : 0.3983\nF1 Score     : 0.0501\n\n=== Error Analysis ===\nTotal errors: 1450\nTop wrong predictions: [('redmi', 217), ('two', 198), ('floral', 89), ('abstract', 53), ('white', 49)]\n\n=== Common Reference Answer Categories ===\nTop reference answers: [('yes', 176), ('plastic', 109), ('black', 99), ('two', 72), ('blue', 53), ('silicone', 49), ('pink', 40), ('three', 39), ('brown', 36), ('white', 34)]\n\n=== Sample Predictions vs References ===\nQuestion : Considering the image and metadata, where would this blanket be most appropriately used?\nReference: Outdoors (Normalized: outdoors)\nPrediction: outdoors (Normalized: outdoors)\nMatch    : True\n---\nQuestion : Based on the image and metadata, where would this product most likely be found in a physical store?\nReference: Kitchen (Normalized: kitchen)\nPrediction: amazon (Normalized: amazon)\nMatch    : False\n---\nQuestion : What is the color of the shoe?\nReference: Grey (Normalized: grey)\nPrediction: brown (Normalized: brown)\nMatch    : False\n---\nQuestion : Considering the image and metadata, what is the primary design feature of this phone case?\nReference: Birds (Normalized: birds)\nPrediction: abstract (Normalized: abstract)\nMatch    : False\n---\nQuestion : Is this binder designed for heavy-duty use or everyday use?\nReference: Everyday (Normalized: everyday)\nPrediction: soft (Normalized: soft)\nMatch    : False\n---\nQuestion : What phone model is this case specifically designed for?\nReference: Realme (Normalized: realme)\nPrediction: redmi (Normalized: redmi)\nMatch    : False\n---\nQuestion : What material is the case made of?\nReference: Silicone (Normalized: silicone)\nPrediction: silicone (Normalized: silicone)\nMatch    : True\n---\nQuestion : What is the overall style of the handle?\nReference: Minimalist (Normalized: minimalist)\nPrediction: modern (Normalized: modern)\nMatch    : False\n---\nQuestion : What is the main color of the motorcycle on the case?\nReference: Gray (Normalized: gray)\nPrediction: black (Normalized: black)\nMatch    : False\n---\nQuestion : What are the two main colors of the mirror frame?\nReference: Two (Normalized: two)\nPrediction: brown (Normalized: brown)\nMatch    : False\n---\nResults saved to test_results_lora_finetuned_improved.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":6}]}